{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhangam11/PyTorch-101-Tutorial-Series/blob/master/Copy_of_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4HI2mpwlrcn"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "679Lmwt3l1Bk",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DSPCom-KmApV"
      },
      "source": [
        "# Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "klAltGp8ycek"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/cnn\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/images/cnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qLGkt5qiyz4E"
      },
      "source": [
        "This tutorial demonstrates training a simple [Convolutional Neural Network](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network) (CNN) to classify [CIFAR images](https://www.cs.toronto.edu/~kriz/cifar.html). Because this tutorial uses the [Keras Sequential API](https://www.tensorflow.org/guide/keras/overview), creating and training our model will take just a few lines of code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m7KBpffWzlxH"
      },
      "source": [
        "### Import TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAve6DCL4JH4",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_DSd8dqNIM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jRFxccghyMVo"
      },
      "source": [
        "### Download and prepare the CIFAR10 dataset\n",
        "\n",
        "\n",
        "The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JWoEqyMuXFF4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "59bd3a25-85c1-44d3-e5b4-fe89862523d4"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0wbL7o0GhzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45cb8583-44c3-4571-beb8-6bfe2cfadc5e"
      },
      "source": [
        "len(train_images)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7wArwCTJJlUa"
      },
      "source": [
        "### Verify the data\n",
        "\n",
        "To verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K3PAELE2eSU9",
        "colab": {}
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oewp-wYg31t9"
      },
      "source": [
        "### Create the convolutional base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hQvqXpNyN3x"
      },
      "source": [
        "The 6 lines of code below define the convolutional base using a common pattern: a stack of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers.\n",
        "\n",
        "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure our CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument `input_shape` to our first layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9YmGQBQPrdn",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX2x3ugVsDg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdExNuRGp4zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(96, (5, 5), activation='relu', input_shape=(32, 32, 3),padding='same',use_bias=True))\n",
        "model.add(layers.Conv2D(96, (5, 5), activation='relu',padding='same'))\n",
        "model.add(layers.MaxPooling2D((3, 3),strides=2))\n",
        "model.add(layers.Conv2D(96, (5, 5), activation='relu', padding='same'))\n",
        "model.add(layers.Conv2D(96, (5, 5), activation='relu',padding='same'))\n",
        "model.add(layers.MaxPooling2D((3, 3),strides=2))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(192, activation='relu'))\n",
        "model.add(layers.Dense(10,activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lvDVFkg-2DPm"
      },
      "source": [
        "Let's display the architecture of our model so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-C4XBg4UTJy",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_j-AXYeZ2GO5"
      },
      "source": [
        "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically,  as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_v8sVOtG37bT"
      },
      "source": [
        "### Add Dense layers on top\n",
        "To complete our model, you will feed the last output tensor from the convolutional base (of shape (4, 4, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D,  then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs and a softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mRs95d6LUVEi",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ipGiQMcR4Gtq"
      },
      "source": [
        "Here's the complete architecture of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Yu_m-TZUWGX",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib. pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvRrUQV_HhyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model():\n",
        "  model = Sequential()  \n",
        "  model.add(Conv2D(16, (3, 3), padding =\n",
        "      'same', activation = 'relu',\n",
        "      input_shape = (32, 32, 3)))\n",
        "  model.add(Conv2D(32, (3, 3), padding =\n",
        "      'same', activation = 'relu',\n",
        "      input_shape = (32, 32, 3)))\n",
        "  model.add(MaxPooling2D(pool_size = (2,\n",
        "          2), strides = None,\n",
        "      padding = 'valid',\n",
        "      data_format = None))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(32, (3, 3), padding =\n",
        "      'same', activation = 'relu',\n",
        "      input_shape = (32, 32, 3)))\n",
        "  model.add(Conv2D(64, (3, 3), padding =\n",
        "      'same', activation = 'relu',\n",
        "      input_shape = (32, 32, 3)))\n",
        "  model.add(MaxPooling2D(pool_size = (2,\n",
        "          2), strides = None,\n",
        "      padding = 'valid',\n",
        "      data_format = None))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, input_shape = (256, )))\n",
        "  model.add(Dense(NUM_CLASSES))\n",
        "  model.add(LeakyReLU(0.1))\n",
        "  model.add(Activation('softmax')) \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHXWM7EXJVkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "k =1 \n",
        "c= 32 *k \n",
        "test_dict = {}\n",
        "test_error={}\n",
        "train_dict = {}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm29CQkXIsN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "92c24f17-ba06-458b-bf1c-a1513ae32e98"
      },
      "source": [
        "\n",
        "c=32\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(c, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(2*c, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten()) # Fl8cattening the 2D arrays for fully connected layers\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(10,activation='softmax'))\n",
        "model.summary()\n",
        "    \n",
        "    \n",
        "model.compile(optimizer='adam',\n",
        "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),#'sparse_categorical_crossentropy',#\n",
        "                  metrics=['accuracy',])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=2,validation_data=(test_images,test_labels)\n",
        "                    )\n",
        "    \n",
        "    \n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2,batch_size=128)\n",
        "print(c,test_loss,test_acc)\n",
        "test_dict[c]=test_loss\n",
        "test_error[c]=1-test_acc\n",
        "train_dict[c]=history.history['loss'][-1]\n",
        "train_error[c]=history.history['accuracy'][-1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 620,362\n",
            "Trainable params: 620,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0975 - accuracy: 0.3551 - val_loss: 2.0295 - val_accuracy: 0.4247\n",
            "Epoch 2/2\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9866 - accuracy: 0.4696 - val_loss: 1.9526 - val_accuracy: 0.5036\n",
            "79/79 - 0s - loss: 1.9512 - accuracy: 0.5036\n",
            "32 1.951234221458435 0.503600001335144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-da389f27debb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_error' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5BIDQNtMeFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 10\n",
        "x_train2 = (train_images/255) - 0.5\n",
        "x_test2 = (test_images/255) - 0.5\n",
        "y_train2 = keras.utils.to_categorical(train_labels,NUM_CLASSES)\n",
        "y_test2 = keras.utils.to_categorical(test_labels,NUM_CLASSES)\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten , Dense, Activation,Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJ4d5FiOPC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY4ZnCvNCizt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "50fb6dbe-ab78-4a7b-cd3c-93bc93665281"
      },
      "source": [
        "\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "#plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 1s - loss: 1.9526 - accuracy: 0.5036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW2Wd5P7CQyY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aac7a819-2060-4d8c-87f4-cc6ee94fc4d8"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64  # orig paper trained all networks with batch_size=128\n",
        "epochs = 300\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 21\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history1 = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history1= model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "    \n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "Learning rate:  0.001\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 32, 32, 16)   448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 32, 32, 16)   64          conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 16)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 32, 32, 16)   2320        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 32, 32, 16)   64          conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 16)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 16)   2320        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 32, 32, 16)   64          conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 32, 32, 16)   0           activation_20[0][0]              \n",
            "                                                                 batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 32, 32, 16)   0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 16)   2320        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 32, 32, 16)   64          conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 32, 32, 16)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 32, 32, 16)   2320        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 16)   64          conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 32, 32, 16)   0           activation_22[0][0]              \n",
            "                                                                 batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 32, 32, 16)   0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 32, 32, 16)   2320        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 16)   64          conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 32, 32, 16)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 32, 32, 16)   2320        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 32, 32, 16)   64          conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 32, 32, 16)   0           activation_24[0][0]              \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 32, 32, 16)   0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 32)   4640        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 32)   9248        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 16, 16, 32)   544         activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 32)   128         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 16, 16, 32)   0           conv2d_31[0][0]                  \n",
            "                                                                 batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 32)   0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 16, 16, 32)   9248        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 32)   128         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 32)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 16, 16, 32)   9248        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 32)   0           activation_28[0][0]              \n",
            "                                                                 batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 32)   0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 32)   9248        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 16, 16, 32)   128         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 32)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 32)   9248        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 16, 16, 32)   128         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 16, 16, 32)   0           activation_30[0][0]              \n",
            "                                                                 batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 16, 16, 32)   0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 64)     18496       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 64)     256         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 64)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 64)     36928       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 64)     2112        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 64)     256         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 8, 8, 64)     0           conv2d_38[0][0]                  \n",
            "                                                                 batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 64)     0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 64)     0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 64)     36928       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 8, 8, 64)     0           activation_34[0][0]              \n",
            "                                                                 batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 64)     0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 64)     0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36928       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 8, 8, 64)     0           activation_36[0][0]              \n",
            "                                                                 batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 64)     0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 64)     0           activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 64)           0           average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           650         flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 274,442\n",
            "Trainable params: 273,066\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n",
            "ResNet20v1\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 1.5965 - accuracy: 0.4761 - val_loss: 1.5496 - val_accuracy: 0.5076\n",
            "Epoch 2/300\n",
            "Learning rate:  0.001\n",
            "  3/782 [..............................] - ETA: 50s - loss: 1.2386 - accuracy: 0.6042"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 48s 62ms/step - loss: 1.2109 - accuracy: 0.6208 - val_loss: 1.2125 - val_accuracy: 0.6248\n",
            "Epoch 3/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 1.0358 - accuracy: 0.6861 - val_loss: 1.0683 - val_accuracy: 0.6774\n",
            "Epoch 4/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.9357 - accuracy: 0.7248 - val_loss: 1.1430 - val_accuracy: 0.6693\n",
            "Epoch 5/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.8592 - accuracy: 0.7533 - val_loss: 0.9586 - val_accuracy: 0.7258\n",
            "Epoch 6/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.8066 - accuracy: 0.7704 - val_loss: 1.0274 - val_accuracy: 0.7111\n",
            "Epoch 7/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.7742 - accuracy: 0.7881 - val_loss: 1.0444 - val_accuracy: 0.7074\n",
            "Epoch 8/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.7308 - accuracy: 0.8023 - val_loss: 1.0897 - val_accuracy: 0.7019\n",
            "Epoch 9/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.7079 - accuracy: 0.8084 - val_loss: 0.8169 - val_accuracy: 0.7813\n",
            "Epoch 10/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.6833 - accuracy: 0.8172 - val_loss: 0.8897 - val_accuracy: 0.7521\n",
            "Epoch 11/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.6694 - accuracy: 0.8232 - val_loss: 0.9183 - val_accuracy: 0.7629\n",
            "Epoch 12/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6463 - accuracy: 0.8325 - val_loss: 0.8187 - val_accuracy: 0.7818\n",
            "Epoch 13/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6300 - accuracy: 0.8368 - val_loss: 0.7742 - val_accuracy: 0.7943\n",
            "Epoch 14/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6187 - accuracy: 0.8418 - val_loss: 0.7992 - val_accuracy: 0.7953\n",
            "Epoch 15/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5994 - accuracy: 0.8480 - val_loss: 0.8314 - val_accuracy: 0.7884\n",
            "Epoch 16/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5918 - accuracy: 0.8509 - val_loss: 0.7383 - val_accuracy: 0.8163\n",
            "Epoch 17/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5808 - accuracy: 0.8566 - val_loss: 0.9661 - val_accuracy: 0.7510\n",
            "Epoch 18/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5705 - accuracy: 0.8598 - val_loss: 0.7966 - val_accuracy: 0.8000\n",
            "Epoch 19/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5654 - accuracy: 0.8628 - val_loss: 0.9218 - val_accuracy: 0.7589\n",
            "Epoch 20/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5591 - accuracy: 0.8648 - val_loss: 0.6689 - val_accuracy: 0.8306\n",
            "Epoch 21/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5485 - accuracy: 0.8696 - val_loss: 0.7750 - val_accuracy: 0.7972\n",
            "Epoch 22/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5402 - accuracy: 0.8717 - val_loss: 0.8096 - val_accuracy: 0.7934\n",
            "Epoch 23/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5328 - accuracy: 0.8754 - val_loss: 0.9921 - val_accuracy: 0.7467\n",
            "Epoch 24/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5346 - accuracy: 0.8742 - val_loss: 0.7191 - val_accuracy: 0.8170\n",
            "Epoch 25/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5243 - accuracy: 0.8774 - val_loss: 0.6938 - val_accuracy: 0.8240\n",
            "Epoch 26/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5161 - accuracy: 0.8800 - val_loss: 0.8009 - val_accuracy: 0.8030\n",
            "Epoch 27/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.5135 - accuracy: 0.8800 - val_loss: 0.7270 - val_accuracy: 0.8259\n",
            "Epoch 28/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.5063 - accuracy: 0.8847 - val_loss: 0.7590 - val_accuracy: 0.8097\n",
            "Epoch 29/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5047 - accuracy: 0.8850 - val_loss: 0.6641 - val_accuracy: 0.8404\n",
            "Epoch 30/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.5020 - accuracy: 0.8867 - val_loss: 0.8347 - val_accuracy: 0.7997\n",
            "Epoch 31/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.4984 - accuracy: 0.8877 - val_loss: 0.7357 - val_accuracy: 0.8240\n",
            "Epoch 32/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.4951 - accuracy: 0.8898 - val_loss: 0.8300 - val_accuracy: 0.8085\n",
            "Epoch 33/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.4943 - accuracy: 0.8897 - val_loss: 0.6352 - val_accuracy: 0.8500\n",
            "Epoch 34/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.4908 - accuracy: 0.8916 - val_loss: 0.7403 - val_accuracy: 0.8213\n",
            "Epoch 35/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.4807 - accuracy: 0.8945 - val_loss: 0.7998 - val_accuracy: 0.7999\n",
            "Epoch 36/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.4834 - accuracy: 0.8937 - val_loss: 0.7039 - val_accuracy: 0.8311\n",
            "Epoch 37/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.4796 - accuracy: 0.8937 - val_loss: 0.6907 - val_accuracy: 0.8354\n",
            "Epoch 38/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.4777 - accuracy: 0.8969 - val_loss: 0.6988 - val_accuracy: 0.8321\n",
            "Epoch 39/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4727 - accuracy: 0.8980 - val_loss: 0.6849 - val_accuracy: 0.8408\n",
            "Epoch 40/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4776 - accuracy: 0.8958 - val_loss: 0.7414 - val_accuracy: 0.8290\n",
            "Epoch 41/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4740 - accuracy: 0.8967 - val_loss: 0.7322 - val_accuracy: 0.8337\n",
            "Epoch 42/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.4712 - accuracy: 0.8998 - val_loss: 0.6802 - val_accuracy: 0.8371\n",
            "Epoch 43/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4637 - accuracy: 0.9020 - val_loss: 0.6612 - val_accuracy: 0.8445\n",
            "Epoch 44/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.4652 - accuracy: 0.9020 - val_loss: 0.6671 - val_accuracy: 0.8456\n",
            "Epoch 45/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4603 - accuracy: 0.9034 - val_loss: 0.6873 - val_accuracy: 0.8436\n",
            "Epoch 46/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4578 - accuracy: 0.9035 - val_loss: 0.6926 - val_accuracy: 0.8380\n",
            "Epoch 47/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4566 - accuracy: 0.9047 - val_loss: 0.6404 - val_accuracy: 0.8566\n",
            "Epoch 48/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4577 - accuracy: 0.9048 - val_loss: 0.6630 - val_accuracy: 0.8466\n",
            "Epoch 49/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4546 - accuracy: 0.9045 - val_loss: 0.7388 - val_accuracy: 0.8285\n",
            "Epoch 50/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4556 - accuracy: 0.9063 - val_loss: 0.7165 - val_accuracy: 0.8358\n",
            "Epoch 51/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4548 - accuracy: 0.9056 - val_loss: 0.6568 - val_accuracy: 0.8509\n",
            "Epoch 52/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4498 - accuracy: 0.9061 - val_loss: 0.6416 - val_accuracy: 0.8623\n",
            "Epoch 53/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.4457 - accuracy: 0.9084 - val_loss: 0.6625 - val_accuracy: 0.8544\n",
            "Epoch 54/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.4458 - accuracy: 0.9074 - val_loss: 0.6698 - val_accuracy: 0.8442\n",
            "Epoch 55/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.4491 - accuracy: 0.9076 - val_loss: 0.6302 - val_accuracy: 0.8578\n",
            "Epoch 56/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.4472 - accuracy: 0.9086 - val_loss: 0.7081 - val_accuracy: 0.8356\n",
            "Epoch 57/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4387 - accuracy: 0.9105 - val_loss: 0.6077 - val_accuracy: 0.8663\n",
            "Epoch 58/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4400 - accuracy: 0.9099 - val_loss: 0.6793 - val_accuracy: 0.8517\n",
            "Epoch 59/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4415 - accuracy: 0.9106 - val_loss: 0.6129 - val_accuracy: 0.8636\n",
            "Epoch 60/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4404 - accuracy: 0.9109 - val_loss: 0.8565 - val_accuracy: 0.8054\n",
            "Epoch 61/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4332 - accuracy: 0.9135 - val_loss: 0.7078 - val_accuracy: 0.8465\n",
            "Epoch 62/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4427 - accuracy: 0.9102 - val_loss: 0.6122 - val_accuracy: 0.8680\n",
            "Epoch 63/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4336 - accuracy: 0.9123 - val_loss: 0.7267 - val_accuracy: 0.8368\n",
            "Epoch 64/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4373 - accuracy: 0.9124 - val_loss: 0.6651 - val_accuracy: 0.8445\n",
            "Epoch 65/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4336 - accuracy: 0.9137 - val_loss: 0.5771 - val_accuracy: 0.8718\n",
            "Epoch 66/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4351 - accuracy: 0.9129 - val_loss: 0.7637 - val_accuracy: 0.8271\n",
            "Epoch 67/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4339 - accuracy: 0.9129 - val_loss: 0.6517 - val_accuracy: 0.8558\n",
            "Epoch 68/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4332 - accuracy: 0.9136 - val_loss: 0.6565 - val_accuracy: 0.8539\n",
            "Epoch 69/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4347 - accuracy: 0.9129 - val_loss: 0.7345 - val_accuracy: 0.8306\n",
            "Epoch 70/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4285 - accuracy: 0.9150 - val_loss: 0.6233 - val_accuracy: 0.8587\n",
            "Epoch 71/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4287 - accuracy: 0.9161 - val_loss: 0.7442 - val_accuracy: 0.8336\n",
            "Epoch 72/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4253 - accuracy: 0.9163 - val_loss: 0.6442 - val_accuracy: 0.8587\n",
            "Epoch 73/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4276 - accuracy: 0.9166 - val_loss: 0.6823 - val_accuracy: 0.8503\n",
            "Epoch 74/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4226 - accuracy: 0.9171 - val_loss: 0.6500 - val_accuracy: 0.8588\n",
            "Epoch 75/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4248 - accuracy: 0.9160 - val_loss: 0.8014 - val_accuracy: 0.8202\n",
            "Epoch 76/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4247 - accuracy: 0.9182 - val_loss: 0.6008 - val_accuracy: 0.8693\n",
            "Epoch 77/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4192 - accuracy: 0.9181 - val_loss: 0.7305 - val_accuracy: 0.8309\n",
            "Epoch 78/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4241 - accuracy: 0.9166 - val_loss: 0.6353 - val_accuracy: 0.8612\n",
            "Epoch 79/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.4217 - accuracy: 0.9182 - val_loss: 0.6539 - val_accuracy: 0.8454\n",
            "Epoch 80/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4232 - accuracy: 0.9187 - val_loss: 0.7203 - val_accuracy: 0.8422\n",
            "Epoch 81/300\n",
            "Learning rate:  0.001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.4210 - accuracy: 0.9182 - val_loss: 0.6083 - val_accuracy: 0.8676\n",
            "Epoch 82/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.3471 - accuracy: 0.9444 - val_loss: 0.4820 - val_accuracy: 0.9037\n",
            "Epoch 83/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.3214 - accuracy: 0.9530 - val_loss: 0.4801 - val_accuracy: 0.9076\n",
            "Epoch 84/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.3087 - accuracy: 0.9568 - val_loss: 0.4744 - val_accuracy: 0.9067\n",
            "Epoch 85/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.2973 - accuracy: 0.9598 - val_loss: 0.4859 - val_accuracy: 0.9058\n",
            "Epoch 86/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.2910 - accuracy: 0.9616 - val_loss: 0.4699 - val_accuracy: 0.9066\n",
            "Epoch 87/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.2847 - accuracy: 0.9622 - val_loss: 0.4727 - val_accuracy: 0.9059\n",
            "Epoch 88/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.2781 - accuracy: 0.9642 - val_loss: 0.4687 - val_accuracy: 0.9064\n",
            "Epoch 89/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.2723 - accuracy: 0.9665 - val_loss: 0.4912 - val_accuracy: 0.9027\n",
            "Epoch 90/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.2668 - accuracy: 0.9664 - val_loss: 0.4779 - val_accuracy: 0.9076\n",
            "Epoch 91/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.2643 - accuracy: 0.9670 - val_loss: 0.4619 - val_accuracy: 0.9095\n",
            "Epoch 92/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.2594 - accuracy: 0.9681 - val_loss: 0.4668 - val_accuracy: 0.9106\n",
            "Epoch 93/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.2563 - accuracy: 0.9686 - val_loss: 0.4729 - val_accuracy: 0.9091\n",
            "Epoch 94/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 63ms/step - loss: 0.2499 - accuracy: 0.9709 - val_loss: 0.4859 - val_accuracy: 0.9045\n",
            "Epoch 95/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2486 - accuracy: 0.9705 - val_loss: 0.4706 - val_accuracy: 0.9071\n",
            "Epoch 96/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 63ms/step - loss: 0.2420 - accuracy: 0.9718 - val_loss: 0.4839 - val_accuracy: 0.9068\n",
            "Epoch 97/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 63ms/step - loss: 0.2413 - accuracy: 0.9718 - val_loss: 0.4665 - val_accuracy: 0.9094\n",
            "Epoch 98/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2351 - accuracy: 0.9728 - val_loss: 0.4748 - val_accuracy: 0.9086\n",
            "Epoch 99/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2341 - accuracy: 0.9733 - val_loss: 0.4741 - val_accuracy: 0.9064\n",
            "Epoch 100/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2298 - accuracy: 0.9744 - val_loss: 0.4735 - val_accuracy: 0.9083\n",
            "Epoch 101/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2289 - accuracy: 0.9749 - val_loss: 0.4741 - val_accuracy: 0.9078\n",
            "Epoch 102/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2228 - accuracy: 0.9758 - val_loss: 0.4889 - val_accuracy: 0.9083\n",
            "Epoch 103/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2220 - accuracy: 0.9753 - val_loss: 0.4743 - val_accuracy: 0.9085\n",
            "Epoch 104/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2211 - accuracy: 0.9751 - val_loss: 0.4755 - val_accuracy: 0.9062\n",
            "Epoch 105/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2169 - accuracy: 0.9762 - val_loss: 0.4808 - val_accuracy: 0.9072\n",
            "Epoch 106/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2137 - accuracy: 0.9773 - val_loss: 0.4787 - val_accuracy: 0.9069\n",
            "Epoch 107/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 65ms/step - loss: 0.2120 - accuracy: 0.9778 - val_loss: 0.4719 - val_accuracy: 0.9088\n",
            "Epoch 108/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2103 - accuracy: 0.9773 - val_loss: 0.4767 - val_accuracy: 0.9089\n",
            "Epoch 109/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2076 - accuracy: 0.9780 - val_loss: 0.4768 - val_accuracy: 0.9056\n",
            "Epoch 110/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2079 - accuracy: 0.9773 - val_loss: 0.4753 - val_accuracy: 0.9108\n",
            "Epoch 111/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2022 - accuracy: 0.9793 - val_loss: 0.4744 - val_accuracy: 0.9101\n",
            "Epoch 112/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.2017 - accuracy: 0.9781 - val_loss: 0.4691 - val_accuracy: 0.9100\n",
            "Epoch 113/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.2002 - accuracy: 0.9795 - val_loss: 0.4792 - val_accuracy: 0.9093\n",
            "Epoch 114/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1976 - accuracy: 0.9799 - val_loss: 0.4754 - val_accuracy: 0.9087\n",
            "Epoch 115/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1946 - accuracy: 0.9803 - val_loss: 0.4836 - val_accuracy: 0.9081\n",
            "Epoch 116/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1957 - accuracy: 0.9793 - val_loss: 0.4883 - val_accuracy: 0.9081\n",
            "Epoch 117/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1930 - accuracy: 0.9801 - val_loss: 0.4875 - val_accuracy: 0.9070\n",
            "Epoch 118/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1900 - accuracy: 0.9812 - val_loss: 0.4986 - val_accuracy: 0.9069\n",
            "Epoch 119/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1865 - accuracy: 0.9819 - val_loss: 0.4801 - val_accuracy: 0.9095\n",
            "Epoch 120/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1867 - accuracy: 0.9818 - val_loss: 0.4858 - val_accuracy: 0.9081\n",
            "Epoch 121/300\n",
            "Learning rate:  0.0001\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1867 - accuracy: 0.9804 - val_loss: 0.4994 - val_accuracy: 0.9057\n",
            "Epoch 122/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 65ms/step - loss: 0.1811 - accuracy: 0.9826 - val_loss: 0.4807 - val_accuracy: 0.9094\n",
            "Epoch 123/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1771 - accuracy: 0.9851 - val_loss: 0.4795 - val_accuracy: 0.9097\n",
            "Epoch 124/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1757 - accuracy: 0.9851 - val_loss: 0.4792 - val_accuracy: 0.9094\n",
            "Epoch 125/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1743 - accuracy: 0.9853 - val_loss: 0.4769 - val_accuracy: 0.9111\n",
            "Epoch 126/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1745 - accuracy: 0.9852 - val_loss: 0.4769 - val_accuracy: 0.9107\n",
            "Epoch 127/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 65ms/step - loss: 0.1738 - accuracy: 0.9862 - val_loss: 0.4786 - val_accuracy: 0.9108\n",
            "Epoch 128/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1711 - accuracy: 0.9863 - val_loss: 0.4777 - val_accuracy: 0.9104\n",
            "Epoch 129/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1716 - accuracy: 0.9871 - val_loss: 0.4752 - val_accuracy: 0.9114\n",
            "Epoch 130/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1718 - accuracy: 0.9864 - val_loss: 0.4786 - val_accuracy: 0.9106\n",
            "Epoch 131/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1711 - accuracy: 0.9864 - val_loss: 0.4783 - val_accuracy: 0.9112\n",
            "Epoch 132/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 65ms/step - loss: 0.1722 - accuracy: 0.9860 - val_loss: 0.4791 - val_accuracy: 0.9111\n",
            "Epoch 133/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1717 - accuracy: 0.9865 - val_loss: 0.4770 - val_accuracy: 0.9115\n",
            "Epoch 134/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1696 - accuracy: 0.9873 - val_loss: 0.4799 - val_accuracy: 0.9106\n",
            "Epoch 135/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1689 - accuracy: 0.9875 - val_loss: 0.4781 - val_accuracy: 0.9110\n",
            "Epoch 136/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1687 - accuracy: 0.9872 - val_loss: 0.4784 - val_accuracy: 0.9110\n",
            "Epoch 137/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1696 - accuracy: 0.9867 - val_loss: 0.4803 - val_accuracy: 0.9102\n",
            "Epoch 138/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1685 - accuracy: 0.9872 - val_loss: 0.4798 - val_accuracy: 0.9101\n",
            "Epoch 139/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1686 - accuracy: 0.9866 - val_loss: 0.4784 - val_accuracy: 0.9113\n",
            "Epoch 140/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1688 - accuracy: 0.9868 - val_loss: 0.4793 - val_accuracy: 0.9096\n",
            "Epoch 141/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1687 - accuracy: 0.9865 - val_loss: 0.4788 - val_accuracy: 0.9100\n",
            "Epoch 142/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1681 - accuracy: 0.9875 - val_loss: 0.4776 - val_accuracy: 0.9101\n",
            "Epoch 143/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1690 - accuracy: 0.9866 - val_loss: 0.4800 - val_accuracy: 0.9100\n",
            "Epoch 144/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1673 - accuracy: 0.9874 - val_loss: 0.4803 - val_accuracy: 0.9107\n",
            "Epoch 145/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1662 - accuracy: 0.9875 - val_loss: 0.4804 - val_accuracy: 0.9102\n",
            "Epoch 146/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 65ms/step - loss: 0.1666 - accuracy: 0.9876 - val_loss: 0.4819 - val_accuracy: 0.9102\n",
            "Epoch 147/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1677 - accuracy: 0.9867 - val_loss: 0.4773 - val_accuracy: 0.9109\n",
            "Epoch 148/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1668 - accuracy: 0.9864 - val_loss: 0.4796 - val_accuracy: 0.9103\n",
            "Epoch 149/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1653 - accuracy: 0.9880 - val_loss: 0.4831 - val_accuracy: 0.9090\n",
            "Epoch 150/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1654 - accuracy: 0.9883 - val_loss: 0.4799 - val_accuracy: 0.9105\n",
            "Epoch 151/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1656 - accuracy: 0.9874 - val_loss: 0.4810 - val_accuracy: 0.9100\n",
            "Epoch 152/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1633 - accuracy: 0.9884 - val_loss: 0.4818 - val_accuracy: 0.9099\n",
            "Epoch 153/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1645 - accuracy: 0.9879 - val_loss: 0.4787 - val_accuracy: 0.9106\n",
            "Epoch 154/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1648 - accuracy: 0.9872 - val_loss: 0.4810 - val_accuracy: 0.9094\n",
            "Epoch 155/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1648 - accuracy: 0.9872 - val_loss: 0.4827 - val_accuracy: 0.9098\n",
            "Epoch 156/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1619 - accuracy: 0.9890 - val_loss: 0.4826 - val_accuracy: 0.9090\n",
            "Epoch 157/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 52s 67ms/step - loss: 0.1632 - accuracy: 0.9885 - val_loss: 0.4794 - val_accuracy: 0.9103\n",
            "Epoch 158/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.1632 - accuracy: 0.9884 - val_loss: 0.4841 - val_accuracy: 0.9103\n",
            "Epoch 159/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.1608 - accuracy: 0.9890 - val_loss: 0.4804 - val_accuracy: 0.9094\n",
            "Epoch 160/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.1634 - accuracy: 0.9876 - val_loss: 0.4837 - val_accuracy: 0.9092\n",
            "Epoch 161/300\n",
            "Learning rate:  1e-05\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.1626 - accuracy: 0.9880 - val_loss: 0.4823 - val_accuracy: 0.9104\n",
            "Epoch 162/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 67ms/step - loss: 0.1613 - accuracy: 0.9888 - val_loss: 0.4819 - val_accuracy: 0.9105\n",
            "Epoch 163/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 66ms/step - loss: 0.1631 - accuracy: 0.9881 - val_loss: 0.4819 - val_accuracy: 0.9103\n",
            "Epoch 164/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 51s 66ms/step - loss: 0.1614 - accuracy: 0.9884 - val_loss: 0.4815 - val_accuracy: 0.9107\n",
            "Epoch 165/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1617 - accuracy: 0.9882 - val_loss: 0.4830 - val_accuracy: 0.9102\n",
            "Epoch 166/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1631 - accuracy: 0.9878 - val_loss: 0.4809 - val_accuracy: 0.9097\n",
            "Epoch 167/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1632 - accuracy: 0.9884 - val_loss: 0.4826 - val_accuracy: 0.9099\n",
            "Epoch 168/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1620 - accuracy: 0.9886 - val_loss: 0.4802 - val_accuracy: 0.9107\n",
            "Epoch 169/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1597 - accuracy: 0.9893 - val_loss: 0.4816 - val_accuracy: 0.9099\n",
            "Epoch 170/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1614 - accuracy: 0.9886 - val_loss: 0.4815 - val_accuracy: 0.9097\n",
            "Epoch 171/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1619 - accuracy: 0.9883 - val_loss: 0.4804 - val_accuracy: 0.9101\n",
            "Epoch 172/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1620 - accuracy: 0.9878 - val_loss: 0.4823 - val_accuracy: 0.9094\n",
            "Epoch 173/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1621 - accuracy: 0.9883 - val_loss: 0.4819 - val_accuracy: 0.9098\n",
            "Epoch 174/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 66ms/step - loss: 0.1615 - accuracy: 0.9883 - val_loss: 0.4818 - val_accuracy: 0.9099\n",
            "Epoch 175/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 67ms/step - loss: 0.1617 - accuracy: 0.9887 - val_loss: 0.4822 - val_accuracy: 0.9092\n",
            "Epoch 176/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 67ms/step - loss: 0.1614 - accuracy: 0.9882 - val_loss: 0.4816 - val_accuracy: 0.9093\n",
            "Epoch 177/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 67ms/step - loss: 0.1619 - accuracy: 0.9888 - val_loss: 0.4826 - val_accuracy: 0.9095\n",
            "Epoch 178/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 66ms/step - loss: 0.1609 - accuracy: 0.9887 - val_loss: 0.4822 - val_accuracy: 0.9093\n",
            "Epoch 179/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 52s 66ms/step - loss: 0.1621 - accuracy: 0.9879 - val_loss: 0.4813 - val_accuracy: 0.9095\n",
            "Epoch 180/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 51s 66ms/step - loss: 0.1619 - accuracy: 0.9884 - val_loss: 0.4813 - val_accuracy: 0.9096\n",
            "Epoch 181/300\n",
            "Learning rate:  1e-06\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1627 - accuracy: 0.9882 - val_loss: 0.4812 - val_accuracy: 0.9091\n",
            "Epoch 182/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1598 - accuracy: 0.9893 - val_loss: 0.4823 - val_accuracy: 0.9086\n",
            "Epoch 183/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1615 - accuracy: 0.9883 - val_loss: 0.4808 - val_accuracy: 0.9092\n",
            "Epoch 184/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1606 - accuracy: 0.9886 - val_loss: 0.4816 - val_accuracy: 0.9097\n",
            "Epoch 185/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1605 - accuracy: 0.9890 - val_loss: 0.4804 - val_accuracy: 0.9100\n",
            "Epoch 186/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1610 - accuracy: 0.9887 - val_loss: 0.4809 - val_accuracy: 0.9090\n",
            "Epoch 187/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1619 - accuracy: 0.9882 - val_loss: 0.4804 - val_accuracy: 0.9092\n",
            "Epoch 188/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1597 - accuracy: 0.9889 - val_loss: 0.4810 - val_accuracy: 0.9097\n",
            "Epoch 189/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1603 - accuracy: 0.9893 - val_loss: 0.4812 - val_accuracy: 0.9096\n",
            "Epoch 190/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1607 - accuracy: 0.9891 - val_loss: 0.4804 - val_accuracy: 0.9101\n",
            "Epoch 191/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1611 - accuracy: 0.9881 - val_loss: 0.4819 - val_accuracy: 0.9096\n",
            "Epoch 192/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1595 - accuracy: 0.9895 - val_loss: 0.4804 - val_accuracy: 0.9099\n",
            "Epoch 193/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1616 - accuracy: 0.9883 - val_loss: 0.4811 - val_accuracy: 0.9092\n",
            "Epoch 194/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1601 - accuracy: 0.9886 - val_loss: 0.4818 - val_accuracy: 0.9093\n",
            "Epoch 195/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1605 - accuracy: 0.9893 - val_loss: 0.4797 - val_accuracy: 0.9100\n",
            "Epoch 196/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1623 - accuracy: 0.9884 - val_loss: 0.4810 - val_accuracy: 0.9092\n",
            "Epoch 197/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1604 - accuracy: 0.9892 - val_loss: 0.4808 - val_accuracy: 0.9096\n",
            "Epoch 198/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1616 - accuracy: 0.9883 - val_loss: 0.4806 - val_accuracy: 0.9097\n",
            "Epoch 199/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1626 - accuracy: 0.9881 - val_loss: 0.4808 - val_accuracy: 0.9095\n",
            "Epoch 200/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1609 - accuracy: 0.9885 - val_loss: 0.4803 - val_accuracy: 0.9094\n",
            "Epoch 201/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1607 - accuracy: 0.9888 - val_loss: 0.4815 - val_accuracy: 0.9098\n",
            "Epoch 202/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1611 - accuracy: 0.9889 - val_loss: 0.4792 - val_accuracy: 0.9101\n",
            "Epoch 203/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1612 - accuracy: 0.9883 - val_loss: 0.4794 - val_accuracy: 0.9094\n",
            "Epoch 204/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1616 - accuracy: 0.9880 - val_loss: 0.4809 - val_accuracy: 0.9095\n",
            "Epoch 205/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1603 - accuracy: 0.9898 - val_loss: 0.4807 - val_accuracy: 0.9094\n",
            "Epoch 206/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1603 - accuracy: 0.9888 - val_loss: 0.4824 - val_accuracy: 0.9095\n",
            "Epoch 207/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1603 - accuracy: 0.9894 - val_loss: 0.4803 - val_accuracy: 0.9101\n",
            "Epoch 208/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1612 - accuracy: 0.9887 - val_loss: 0.4817 - val_accuracy: 0.9097\n",
            "Epoch 209/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1607 - accuracy: 0.9886 - val_loss: 0.4801 - val_accuracy: 0.9100\n",
            "Epoch 210/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1612 - accuracy: 0.9892 - val_loss: 0.4802 - val_accuracy: 0.9097\n",
            "Epoch 211/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1619 - accuracy: 0.9879 - val_loss: 0.4818 - val_accuracy: 0.9098\n",
            "Epoch 212/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 50s 63ms/step - loss: 0.1606 - accuracy: 0.9887 - val_loss: 0.4812 - val_accuracy: 0.9096\n",
            "Epoch 213/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1609 - accuracy: 0.9892 - val_loss: 0.4819 - val_accuracy: 0.9100\n",
            "Epoch 214/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1612 - accuracy: 0.9887 - val_loss: 0.4802 - val_accuracy: 0.9098\n",
            "Epoch 215/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1613 - accuracy: 0.9884 - val_loss: 0.4822 - val_accuracy: 0.9101\n",
            "Epoch 216/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1603 - accuracy: 0.9891 - val_loss: 0.4787 - val_accuracy: 0.9102\n",
            "Epoch 217/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1595 - accuracy: 0.9894 - val_loss: 0.4799 - val_accuracy: 0.9098\n",
            "Epoch 218/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1606 - accuracy: 0.9893 - val_loss: 0.4809 - val_accuracy: 0.9098\n",
            "Epoch 219/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1615 - accuracy: 0.9883 - val_loss: 0.4818 - val_accuracy: 0.9098\n",
            "Epoch 220/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1611 - accuracy: 0.9889 - val_loss: 0.4810 - val_accuracy: 0.9103\n",
            "Epoch 221/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1608 - accuracy: 0.9888 - val_loss: 0.4812 - val_accuracy: 0.9096\n",
            "Epoch 222/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1599 - accuracy: 0.9893 - val_loss: 0.4818 - val_accuracy: 0.9097\n",
            "Epoch 223/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1604 - accuracy: 0.9888 - val_loss: 0.4801 - val_accuracy: 0.9095\n",
            "Epoch 224/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1608 - accuracy: 0.9889 - val_loss: 0.4814 - val_accuracy: 0.9095\n",
            "Epoch 225/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1600 - accuracy: 0.9887 - val_loss: 0.4819 - val_accuracy: 0.9099\n",
            "Epoch 226/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1605 - accuracy: 0.9888 - val_loss: 0.4809 - val_accuracy: 0.9094\n",
            "Epoch 227/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1606 - accuracy: 0.9890 - val_loss: 0.4815 - val_accuracy: 0.9099\n",
            "Epoch 228/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1612 - accuracy: 0.9889 - val_loss: 0.4810 - val_accuracy: 0.9104\n",
            "Epoch 229/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1600 - accuracy: 0.9893 - val_loss: 0.4811 - val_accuracy: 0.9100\n",
            "Epoch 230/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1601 - accuracy: 0.9892 - val_loss: 0.4820 - val_accuracy: 0.9095\n",
            "Epoch 231/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1609 - accuracy: 0.9888 - val_loss: 0.4817 - val_accuracy: 0.9097\n",
            "Epoch 232/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1586 - accuracy: 0.9893 - val_loss: 0.4817 - val_accuracy: 0.9094\n",
            "Epoch 233/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1607 - accuracy: 0.9889 - val_loss: 0.4814 - val_accuracy: 0.9097\n",
            "Epoch 234/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1594 - accuracy: 0.9893 - val_loss: 0.4810 - val_accuracy: 0.9098\n",
            "Epoch 235/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1606 - accuracy: 0.9889 - val_loss: 0.4820 - val_accuracy: 0.9093\n",
            "Epoch 236/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 51s 65ms/step - loss: 0.1606 - accuracy: 0.9882 - val_loss: 0.4825 - val_accuracy: 0.9098\n",
            "Epoch 237/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.1595 - accuracy: 0.9892 - val_loss: 0.4810 - val_accuracy: 0.9093\n",
            "Epoch 238/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 50s 65ms/step - loss: 0.1600 - accuracy: 0.9890 - val_loss: 0.4806 - val_accuracy: 0.9101\n",
            "Epoch 239/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1606 - accuracy: 0.9887 - val_loss: 0.4814 - val_accuracy: 0.9098\n",
            "Epoch 240/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1597 - accuracy: 0.9894 - val_loss: 0.4820 - val_accuracy: 0.9098\n",
            "Epoch 241/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1623 - accuracy: 0.9881 - val_loss: 0.4822 - val_accuracy: 0.9096\n",
            "Epoch 242/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1632 - accuracy: 0.9874 - val_loss: 0.4821 - val_accuracy: 0.9102\n",
            "Epoch 243/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1594 - accuracy: 0.9894 - val_loss: 0.4813 - val_accuracy: 0.9101\n",
            "Epoch 244/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1622 - accuracy: 0.9877 - val_loss: 0.4821 - val_accuracy: 0.9099\n",
            "Epoch 245/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1606 - accuracy: 0.9886 - val_loss: 0.4830 - val_accuracy: 0.9094\n",
            "Epoch 246/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1614 - accuracy: 0.9886 - val_loss: 0.4813 - val_accuracy: 0.9106\n",
            "Epoch 247/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1612 - accuracy: 0.9884 - val_loss: 0.4818 - val_accuracy: 0.9097\n",
            "Epoch 248/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1601 - accuracy: 0.9893 - val_loss: 0.4819 - val_accuracy: 0.9098\n",
            "Epoch 249/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1598 - accuracy: 0.9892 - val_loss: 0.4813 - val_accuracy: 0.9099\n",
            "Epoch 250/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1613 - accuracy: 0.9886 - val_loss: 0.4819 - val_accuracy: 0.9101\n",
            "Epoch 251/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1601 - accuracy: 0.9886 - val_loss: 0.4809 - val_accuracy: 0.9095\n",
            "Epoch 252/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1592 - accuracy: 0.9892 - val_loss: 0.4815 - val_accuracy: 0.9098\n",
            "Epoch 253/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1591 - accuracy: 0.9897 - val_loss: 0.4809 - val_accuracy: 0.9098\n",
            "Epoch 254/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1602 - accuracy: 0.9891 - val_loss: 0.4812 - val_accuracy: 0.9103\n",
            "Epoch 255/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1594 - accuracy: 0.9890 - val_loss: 0.4815 - val_accuracy: 0.9093\n",
            "Epoch 256/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1601 - accuracy: 0.9886 - val_loss: 0.4819 - val_accuracy: 0.9102\n",
            "Epoch 257/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1601 - accuracy: 0.9889 - val_loss: 0.4815 - val_accuracy: 0.9101\n",
            "Epoch 258/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1606 - accuracy: 0.9886 - val_loss: 0.4808 - val_accuracy: 0.9098\n",
            "Epoch 259/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1599 - accuracy: 0.9892 - val_loss: 0.4832 - val_accuracy: 0.9095\n",
            "Epoch 260/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1609 - accuracy: 0.9885 - val_loss: 0.4813 - val_accuracy: 0.9099\n",
            "Epoch 261/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1604 - accuracy: 0.9888 - val_loss: 0.4835 - val_accuracy: 0.9090\n",
            "Epoch 262/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1599 - accuracy: 0.9887 - val_loss: 0.4809 - val_accuracy: 0.9102\n",
            "Epoch 263/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1602 - accuracy: 0.9889 - val_loss: 0.4808 - val_accuracy: 0.9104\n",
            "Epoch 264/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1597 - accuracy: 0.9892 - val_loss: 0.4834 - val_accuracy: 0.9095\n",
            "Epoch 265/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1601 - accuracy: 0.9889 - val_loss: 0.4804 - val_accuracy: 0.9098\n",
            "Epoch 266/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1602 - accuracy: 0.9889 - val_loss: 0.4817 - val_accuracy: 0.9099\n",
            "Epoch 267/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1581 - accuracy: 0.9896 - val_loss: 0.4814 - val_accuracy: 0.9101\n",
            "Epoch 268/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1608 - accuracy: 0.9883 - val_loss: 0.4825 - val_accuracy: 0.9098\n",
            "Epoch 269/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1596 - accuracy: 0.9894 - val_loss: 0.4814 - val_accuracy: 0.9100\n",
            "Epoch 270/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1588 - accuracy: 0.9891 - val_loss: 0.4817 - val_accuracy: 0.9100\n",
            "Epoch 271/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1610 - accuracy: 0.9886 - val_loss: 0.4813 - val_accuracy: 0.9097\n",
            "Epoch 272/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1587 - accuracy: 0.9892 - val_loss: 0.4815 - val_accuracy: 0.9102\n",
            "Epoch 273/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1581 - accuracy: 0.9898 - val_loss: 0.4809 - val_accuracy: 0.9097\n",
            "Epoch 274/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1602 - accuracy: 0.9893 - val_loss: 0.4810 - val_accuracy: 0.9100\n",
            "Epoch 275/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1605 - accuracy: 0.9888 - val_loss: 0.4810 - val_accuracy: 0.9103\n",
            "Epoch 276/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1599 - accuracy: 0.9892 - val_loss: 0.4828 - val_accuracy: 0.9096\n",
            "Epoch 277/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1589 - accuracy: 0.9892 - val_loss: 0.4822 - val_accuracy: 0.9097\n",
            "Epoch 278/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 47s 61ms/step - loss: 0.1596 - accuracy: 0.9895 - val_loss: 0.4816 - val_accuracy: 0.9097\n",
            "Epoch 279/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1602 - accuracy: 0.9889 - val_loss: 0.4817 - val_accuracy: 0.9100\n",
            "Epoch 280/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 50s 64ms/step - loss: 0.1595 - accuracy: 0.9884 - val_loss: 0.4816 - val_accuracy: 0.9103\n",
            "Epoch 281/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1590 - accuracy: 0.9893 - val_loss: 0.4810 - val_accuracy: 0.9101\n",
            "Epoch 282/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1595 - accuracy: 0.9894 - val_loss: 0.4826 - val_accuracy: 0.9096\n",
            "Epoch 283/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 63ms/step - loss: 0.1610 - accuracy: 0.9886 - val_loss: 0.4814 - val_accuracy: 0.9102\n",
            "Epoch 284/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1596 - accuracy: 0.9889 - val_loss: 0.4826 - val_accuracy: 0.9095\n",
            "Epoch 285/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1603 - accuracy: 0.9891 - val_loss: 0.4835 - val_accuracy: 0.9096\n",
            "Epoch 286/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1581 - accuracy: 0.9897 - val_loss: 0.4815 - val_accuracy: 0.9098\n",
            "Epoch 287/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1601 - accuracy: 0.9891 - val_loss: 0.4830 - val_accuracy: 0.9102\n",
            "Epoch 288/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1604 - accuracy: 0.9886 - val_loss: 0.4813 - val_accuracy: 0.9099\n",
            "Epoch 289/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1583 - accuracy: 0.9898 - val_loss: 0.4812 - val_accuracy: 0.9104\n",
            "Epoch 290/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1593 - accuracy: 0.9895 - val_loss: 0.4812 - val_accuracy: 0.9104\n",
            "Epoch 291/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1604 - accuracy: 0.9888 - val_loss: 0.4822 - val_accuracy: 0.9098\n",
            "Epoch 292/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1577 - accuracy: 0.9902 - val_loss: 0.4823 - val_accuracy: 0.9105\n",
            "Epoch 293/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1595 - accuracy: 0.9890 - val_loss: 0.4827 - val_accuracy: 0.9100\n",
            "Epoch 294/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1598 - accuracy: 0.9891 - val_loss: 0.4816 - val_accuracy: 0.9103\n",
            "Epoch 295/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1606 - accuracy: 0.9890 - val_loss: 0.4807 - val_accuracy: 0.9101\n",
            "Epoch 296/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1592 - accuracy: 0.9894 - val_loss: 0.4816 - val_accuracy: 0.9099\n",
            "Epoch 297/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1601 - accuracy: 0.9888 - val_loss: 0.4816 - val_accuracy: 0.9098\n",
            "Epoch 298/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 49s 62ms/step - loss: 0.1609 - accuracy: 0.9884 - val_loss: 0.4818 - val_accuracy: 0.9100\n",
            "Epoch 299/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 61ms/step - loss: 0.1604 - accuracy: 0.9890 - val_loss: 0.4818 - val_accuracy: 0.9099\n",
            "Epoch 300/300\n",
            "Learning rate:  5e-07\n",
            "782/782 [==============================] - 48s 62ms/step - loss: 0.1596 - accuracy: 0.9886 - val_loss: 0.4812 - val_accuracy: 0.9106\n",
            "10000/10000 [==============================] - 3s 339us/step\n",
            "Test loss: 0.48124655861854554\n",
            "Test accuracy: 0.9106000065803528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmds4ALi4o4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4552911f-f8f3-4511-f751-be3a823e6e74",
        "id": "zx1gCnrf4pmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64  # orig paper trained all networks with batch_size=128\n",
        "epochs = 300\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 27\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history1 = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history1= model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "    \n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "Learning rate:  0.001\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_337 (Conv2D)             (None, 32, 32, 16)   448         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_329 (BatchN (None, 32, 32, 16)   64          conv2d_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_329 (Activation)     (None, 32, 32, 16)   0           batch_normalization_329[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_338 (Conv2D)             (None, 32, 32, 16)   2320        activation_329[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_330 (BatchN (None, 32, 32, 16)   64          conv2d_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_330 (Activation)     (None, 32, 32, 16)   0           batch_normalization_330[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_339 (Conv2D)             (None, 32, 32, 16)   2320        activation_330[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_331 (BatchN (None, 32, 32, 16)   64          conv2d_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_163 (Add)                   (None, 32, 32, 16)   0           activation_329[0][0]             \n",
            "                                                                 batch_normalization_331[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_331 (Activation)     (None, 32, 32, 16)   0           add_163[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_340 (Conv2D)             (None, 32, 32, 16)   2320        activation_331[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_332 (BatchN (None, 32, 32, 16)   64          conv2d_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_332 (Activation)     (None, 32, 32, 16)   0           batch_normalization_332[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_341 (Conv2D)             (None, 32, 32, 16)   2320        activation_332[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_333 (BatchN (None, 32, 32, 16)   64          conv2d_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_164 (Add)                   (None, 32, 32, 16)   0           activation_331[0][0]             \n",
            "                                                                 batch_normalization_333[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_333 (Activation)     (None, 32, 32, 16)   0           add_164[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_342 (Conv2D)             (None, 32, 32, 16)   2320        activation_333[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_334 (BatchN (None, 32, 32, 16)   64          conv2d_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_334 (Activation)     (None, 32, 32, 16)   0           batch_normalization_334[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_343 (Conv2D)             (None, 32, 32, 16)   2320        activation_334[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_335 (BatchN (None, 32, 32, 16)   64          conv2d_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_165 (Add)                   (None, 32, 32, 16)   0           activation_333[0][0]             \n",
            "                                                                 batch_normalization_335[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_335 (Activation)     (None, 32, 32, 16)   0           add_165[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_344 (Conv2D)             (None, 32, 32, 16)   2320        activation_335[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_336 (BatchN (None, 32, 32, 16)   64          conv2d_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_336 (Activation)     (None, 32, 32, 16)   0           batch_normalization_336[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_345 (Conv2D)             (None, 32, 32, 16)   2320        activation_336[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_337 (BatchN (None, 32, 32, 16)   64          conv2d_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_166 (Add)                   (None, 32, 32, 16)   0           activation_335[0][0]             \n",
            "                                                                 batch_normalization_337[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_337 (Activation)     (None, 32, 32, 16)   0           add_166[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_346 (Conv2D)             (None, 32, 32, 16)   2320        activation_337[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_338 (BatchN (None, 32, 32, 16)   64          conv2d_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_338 (Activation)     (None, 32, 32, 16)   0           batch_normalization_338[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_347 (Conv2D)             (None, 32, 32, 16)   2320        activation_338[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_339 (BatchN (None, 32, 32, 16)   64          conv2d_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_167 (Add)                   (None, 32, 32, 16)   0           activation_337[0][0]             \n",
            "                                                                 batch_normalization_339[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_339 (Activation)     (None, 32, 32, 16)   0           add_167[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_348 (Conv2D)             (None, 32, 32, 16)   2320        activation_339[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_340 (BatchN (None, 32, 32, 16)   64          conv2d_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_340 (Activation)     (None, 32, 32, 16)   0           batch_normalization_340[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_349 (Conv2D)             (None, 32, 32, 16)   2320        activation_340[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_341 (BatchN (None, 32, 32, 16)   64          conv2d_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_168 (Add)                   (None, 32, 32, 16)   0           activation_339[0][0]             \n",
            "                                                                 batch_normalization_341[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_341 (Activation)     (None, 32, 32, 16)   0           add_168[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_350 (Conv2D)             (None, 32, 32, 16)   2320        activation_341[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_342 (BatchN (None, 32, 32, 16)   64          conv2d_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_342 (Activation)     (None, 32, 32, 16)   0           batch_normalization_342[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_351 (Conv2D)             (None, 32, 32, 16)   2320        activation_342[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_343 (BatchN (None, 32, 32, 16)   64          conv2d_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_169 (Add)                   (None, 32, 32, 16)   0           activation_341[0][0]             \n",
            "                                                                 batch_normalization_343[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_343 (Activation)     (None, 32, 32, 16)   0           add_169[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_352 (Conv2D)             (None, 32, 32, 16)   2320        activation_343[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_344 (BatchN (None, 32, 32, 16)   64          conv2d_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_344 (Activation)     (None, 32, 32, 16)   0           batch_normalization_344[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_353 (Conv2D)             (None, 32, 32, 16)   2320        activation_344[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_345 (BatchN (None, 32, 32, 16)   64          conv2d_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_170 (Add)                   (None, 32, 32, 16)   0           activation_343[0][0]             \n",
            "                                                                 batch_normalization_345[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_345 (Activation)     (None, 32, 32, 16)   0           add_170[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_354 (Conv2D)             (None, 32, 32, 16)   2320        activation_345[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_346 (BatchN (None, 32, 32, 16)   64          conv2d_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_346 (Activation)     (None, 32, 32, 16)   0           batch_normalization_346[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_355 (Conv2D)             (None, 32, 32, 16)   2320        activation_346[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_347 (BatchN (None, 32, 32, 16)   64          conv2d_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_171 (Add)                   (None, 32, 32, 16)   0           activation_345[0][0]             \n",
            "                                                                 batch_normalization_347[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_347 (Activation)     (None, 32, 32, 16)   0           add_171[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_356 (Conv2D)             (None, 32, 32, 16)   2320        activation_347[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_348 (BatchN (None, 32, 32, 16)   64          conv2d_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_348 (Activation)     (None, 32, 32, 16)   0           batch_normalization_348[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_357 (Conv2D)             (None, 32, 32, 16)   2320        activation_348[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_349 (BatchN (None, 32, 32, 16)   64          conv2d_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_172 (Add)                   (None, 32, 32, 16)   0           activation_347[0][0]             \n",
            "                                                                 batch_normalization_349[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_349 (Activation)     (None, 32, 32, 16)   0           add_172[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_358 (Conv2D)             (None, 32, 32, 16)   2320        activation_349[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_350 (BatchN (None, 32, 32, 16)   64          conv2d_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_350 (Activation)     (None, 32, 32, 16)   0           batch_normalization_350[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_359 (Conv2D)             (None, 32, 32, 16)   2320        activation_350[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_351 (BatchN (None, 32, 32, 16)   64          conv2d_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_173 (Add)                   (None, 32, 32, 16)   0           activation_349[0][0]             \n",
            "                                                                 batch_normalization_351[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_351 (Activation)     (None, 32, 32, 16)   0           add_173[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_360 (Conv2D)             (None, 32, 32, 16)   2320        activation_351[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_352 (BatchN (None, 32, 32, 16)   64          conv2d_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_352 (Activation)     (None, 32, 32, 16)   0           batch_normalization_352[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_361 (Conv2D)             (None, 32, 32, 16)   2320        activation_352[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_353 (BatchN (None, 32, 32, 16)   64          conv2d_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_174 (Add)                   (None, 32, 32, 16)   0           activation_351[0][0]             \n",
            "                                                                 batch_normalization_353[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_353 (Activation)     (None, 32, 32, 16)   0           add_174[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_362 (Conv2D)             (None, 32, 32, 16)   2320        activation_353[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_354 (BatchN (None, 32, 32, 16)   64          conv2d_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_354 (Activation)     (None, 32, 32, 16)   0           batch_normalization_354[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_363 (Conv2D)             (None, 32, 32, 16)   2320        activation_354[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_355 (BatchN (None, 32, 32, 16)   64          conv2d_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_175 (Add)                   (None, 32, 32, 16)   0           activation_353[0][0]             \n",
            "                                                                 batch_normalization_355[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_355 (Activation)     (None, 32, 32, 16)   0           add_175[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_364 (Conv2D)             (None, 32, 32, 16)   2320        activation_355[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_356 (BatchN (None, 32, 32, 16)   64          conv2d_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_356 (Activation)     (None, 32, 32, 16)   0           batch_normalization_356[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_365 (Conv2D)             (None, 32, 32, 16)   2320        activation_356[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_357 (BatchN (None, 32, 32, 16)   64          conv2d_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_176 (Add)                   (None, 32, 32, 16)   0           activation_355[0][0]             \n",
            "                                                                 batch_normalization_357[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_357 (Activation)     (None, 32, 32, 16)   0           add_176[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_366 (Conv2D)             (None, 32, 32, 16)   2320        activation_357[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_358 (BatchN (None, 32, 32, 16)   64          conv2d_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_358 (Activation)     (None, 32, 32, 16)   0           batch_normalization_358[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_367 (Conv2D)             (None, 32, 32, 16)   2320        activation_358[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_359 (BatchN (None, 32, 32, 16)   64          conv2d_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_177 (Add)                   (None, 32, 32, 16)   0           activation_357[0][0]             \n",
            "                                                                 batch_normalization_359[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_359 (Activation)     (None, 32, 32, 16)   0           add_177[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_368 (Conv2D)             (None, 32, 32, 16)   2320        activation_359[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_360 (BatchN (None, 32, 32, 16)   64          conv2d_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_360 (Activation)     (None, 32, 32, 16)   0           batch_normalization_360[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_369 (Conv2D)             (None, 32, 32, 16)   2320        activation_360[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_361 (BatchN (None, 32, 32, 16)   64          conv2d_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_178 (Add)                   (None, 32, 32, 16)   0           activation_359[0][0]             \n",
            "                                                                 batch_normalization_361[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_361 (Activation)     (None, 32, 32, 16)   0           add_178[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_370 (Conv2D)             (None, 32, 32, 16)   2320        activation_361[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_362 (BatchN (None, 32, 32, 16)   64          conv2d_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_362 (Activation)     (None, 32, 32, 16)   0           batch_normalization_362[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_371 (Conv2D)             (None, 32, 32, 16)   2320        activation_362[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_363 (BatchN (None, 32, 32, 16)   64          conv2d_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_179 (Add)                   (None, 32, 32, 16)   0           activation_361[0][0]             \n",
            "                                                                 batch_normalization_363[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_363 (Activation)     (None, 32, 32, 16)   0           add_179[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_372 (Conv2D)             (None, 32, 32, 16)   2320        activation_363[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_364 (BatchN (None, 32, 32, 16)   64          conv2d_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_364 (Activation)     (None, 32, 32, 16)   0           batch_normalization_364[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_373 (Conv2D)             (None, 32, 32, 16)   2320        activation_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_365 (BatchN (None, 32, 32, 16)   64          conv2d_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_180 (Add)                   (None, 32, 32, 16)   0           activation_363[0][0]             \n",
            "                                                                 batch_normalization_365[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_365 (Activation)     (None, 32, 32, 16)   0           add_180[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_374 (Conv2D)             (None, 32, 32, 16)   2320        activation_365[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_366 (BatchN (None, 32, 32, 16)   64          conv2d_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_366 (Activation)     (None, 32, 32, 16)   0           batch_normalization_366[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_375 (Conv2D)             (None, 32, 32, 16)   2320        activation_366[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_367 (BatchN (None, 32, 32, 16)   64          conv2d_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_181 (Add)                   (None, 32, 32, 16)   0           activation_365[0][0]             \n",
            "                                                                 batch_normalization_367[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_367 (Activation)     (None, 32, 32, 16)   0           add_181[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_376 (Conv2D)             (None, 32, 32, 16)   2320        activation_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_368 (BatchN (None, 32, 32, 16)   64          conv2d_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_368 (Activation)     (None, 32, 32, 16)   0           batch_normalization_368[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_377 (Conv2D)             (None, 32, 32, 16)   2320        activation_368[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_369 (BatchN (None, 32, 32, 16)   64          conv2d_377[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_182 (Add)                   (None, 32, 32, 16)   0           activation_367[0][0]             \n",
            "                                                                 batch_normalization_369[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_369 (Activation)     (None, 32, 32, 16)   0           add_182[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_378 (Conv2D)             (None, 32, 32, 16)   2320        activation_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_370 (BatchN (None, 32, 32, 16)   64          conv2d_378[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_370 (Activation)     (None, 32, 32, 16)   0           batch_normalization_370[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_379 (Conv2D)             (None, 32, 32, 16)   2320        activation_370[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_371 (BatchN (None, 32, 32, 16)   64          conv2d_379[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_183 (Add)                   (None, 32, 32, 16)   0           activation_369[0][0]             \n",
            "                                                                 batch_normalization_371[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_371 (Activation)     (None, 32, 32, 16)   0           add_183[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_380 (Conv2D)             (None, 32, 32, 16)   2320        activation_371[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_372 (BatchN (None, 32, 32, 16)   64          conv2d_380[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_372 (Activation)     (None, 32, 32, 16)   0           batch_normalization_372[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_381 (Conv2D)             (None, 32, 32, 16)   2320        activation_372[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_373 (BatchN (None, 32, 32, 16)   64          conv2d_381[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_184 (Add)                   (None, 32, 32, 16)   0           activation_371[0][0]             \n",
            "                                                                 batch_normalization_373[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_373 (Activation)     (None, 32, 32, 16)   0           add_184[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_382 (Conv2D)             (None, 32, 32, 16)   2320        activation_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_374 (BatchN (None, 32, 32, 16)   64          conv2d_382[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_374 (Activation)     (None, 32, 32, 16)   0           batch_normalization_374[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_383 (Conv2D)             (None, 32, 32, 16)   2320        activation_374[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_375 (BatchN (None, 32, 32, 16)   64          conv2d_383[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_185 (Add)                   (None, 32, 32, 16)   0           activation_373[0][0]             \n",
            "                                                                 batch_normalization_375[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_375 (Activation)     (None, 32, 32, 16)   0           add_185[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_384 (Conv2D)             (None, 32, 32, 16)   2320        activation_375[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_376 (BatchN (None, 32, 32, 16)   64          conv2d_384[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_376 (Activation)     (None, 32, 32, 16)   0           batch_normalization_376[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_385 (Conv2D)             (None, 32, 32, 16)   2320        activation_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_377 (BatchN (None, 32, 32, 16)   64          conv2d_385[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_186 (Add)                   (None, 32, 32, 16)   0           activation_375[0][0]             \n",
            "                                                                 batch_normalization_377[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_377 (Activation)     (None, 32, 32, 16)   0           add_186[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_386 (Conv2D)             (None, 32, 32, 16)   2320        activation_377[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_378 (BatchN (None, 32, 32, 16)   64          conv2d_386[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_378 (Activation)     (None, 32, 32, 16)   0           batch_normalization_378[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_387 (Conv2D)             (None, 32, 32, 16)   2320        activation_378[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_379 (BatchN (None, 32, 32, 16)   64          conv2d_387[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_187 (Add)                   (None, 32, 32, 16)   0           activation_377[0][0]             \n",
            "                                                                 batch_normalization_379[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_379 (Activation)     (None, 32, 32, 16)   0           add_187[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_388 (Conv2D)             (None, 32, 32, 16)   2320        activation_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_380 (BatchN (None, 32, 32, 16)   64          conv2d_388[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_380 (Activation)     (None, 32, 32, 16)   0           batch_normalization_380[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_389 (Conv2D)             (None, 32, 32, 16)   2320        activation_380[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_381 (BatchN (None, 32, 32, 16)   64          conv2d_389[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_188 (Add)                   (None, 32, 32, 16)   0           activation_379[0][0]             \n",
            "                                                                 batch_normalization_381[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_381 (Activation)     (None, 32, 32, 16)   0           add_188[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_390 (Conv2D)             (None, 32, 32, 16)   2320        activation_381[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_382 (BatchN (None, 32, 32, 16)   64          conv2d_390[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_382 (Activation)     (None, 32, 32, 16)   0           batch_normalization_382[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_391 (Conv2D)             (None, 32, 32, 16)   2320        activation_382[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_383 (BatchN (None, 32, 32, 16)   64          conv2d_391[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_189 (Add)                   (None, 32, 32, 16)   0           activation_381[0][0]             \n",
            "                                                                 batch_normalization_383[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_383 (Activation)     (None, 32, 32, 16)   0           add_189[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_392 (Conv2D)             (None, 16, 16, 32)   4640        activation_383[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_384 (BatchN (None, 16, 16, 32)   128         conv2d_392[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_384 (Activation)     (None, 16, 16, 32)   0           batch_normalization_384[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_393 (Conv2D)             (None, 16, 16, 32)   9248        activation_384[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_394 (Conv2D)             (None, 16, 16, 32)   544         activation_383[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_385 (BatchN (None, 16, 16, 32)   128         conv2d_393[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_190 (Add)                   (None, 16, 16, 32)   0           conv2d_394[0][0]                 \n",
            "                                                                 batch_normalization_385[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_385 (Activation)     (None, 16, 16, 32)   0           add_190[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_395 (Conv2D)             (None, 16, 16, 32)   9248        activation_385[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_386 (BatchN (None, 16, 16, 32)   128         conv2d_395[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_386 (Activation)     (None, 16, 16, 32)   0           batch_normalization_386[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_396 (Conv2D)             (None, 16, 16, 32)   9248        activation_386[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_387 (BatchN (None, 16, 16, 32)   128         conv2d_396[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_191 (Add)                   (None, 16, 16, 32)   0           activation_385[0][0]             \n",
            "                                                                 batch_normalization_387[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_387 (Activation)     (None, 16, 16, 32)   0           add_191[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_397 (Conv2D)             (None, 16, 16, 32)   9248        activation_387[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_388 (BatchN (None, 16, 16, 32)   128         conv2d_397[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_388 (Activation)     (None, 16, 16, 32)   0           batch_normalization_388[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_398 (Conv2D)             (None, 16, 16, 32)   9248        activation_388[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_389 (BatchN (None, 16, 16, 32)   128         conv2d_398[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_192 (Add)                   (None, 16, 16, 32)   0           activation_387[0][0]             \n",
            "                                                                 batch_normalization_389[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_389 (Activation)     (None, 16, 16, 32)   0           add_192[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_399 (Conv2D)             (None, 16, 16, 32)   9248        activation_389[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_390 (BatchN (None, 16, 16, 32)   128         conv2d_399[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_390 (Activation)     (None, 16, 16, 32)   0           batch_normalization_390[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_400 (Conv2D)             (None, 16, 16, 32)   9248        activation_390[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_391 (BatchN (None, 16, 16, 32)   128         conv2d_400[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_193 (Add)                   (None, 16, 16, 32)   0           activation_389[0][0]             \n",
            "                                                                 batch_normalization_391[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_391 (Activation)     (None, 16, 16, 32)   0           add_193[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_401 (Conv2D)             (None, 16, 16, 32)   9248        activation_391[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_392 (BatchN (None, 16, 16, 32)   128         conv2d_401[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_392 (Activation)     (None, 16, 16, 32)   0           batch_normalization_392[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_402 (Conv2D)             (None, 16, 16, 32)   9248        activation_392[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_393 (BatchN (None, 16, 16, 32)   128         conv2d_402[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_194 (Add)                   (None, 16, 16, 32)   0           activation_391[0][0]             \n",
            "                                                                 batch_normalization_393[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_393 (Activation)     (None, 16, 16, 32)   0           add_194[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_403 (Conv2D)             (None, 16, 16, 32)   9248        activation_393[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_394 (BatchN (None, 16, 16, 32)   128         conv2d_403[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_394 (Activation)     (None, 16, 16, 32)   0           batch_normalization_394[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_404 (Conv2D)             (None, 16, 16, 32)   9248        activation_394[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_395 (BatchN (None, 16, 16, 32)   128         conv2d_404[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_195 (Add)                   (None, 16, 16, 32)   0           activation_393[0][0]             \n",
            "                                                                 batch_normalization_395[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_395 (Activation)     (None, 16, 16, 32)   0           add_195[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_405 (Conv2D)             (None, 16, 16, 32)   9248        activation_395[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_396 (BatchN (None, 16, 16, 32)   128         conv2d_405[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_396 (Activation)     (None, 16, 16, 32)   0           batch_normalization_396[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_406 (Conv2D)             (None, 16, 16, 32)   9248        activation_396[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_397 (BatchN (None, 16, 16, 32)   128         conv2d_406[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_196 (Add)                   (None, 16, 16, 32)   0           activation_395[0][0]             \n",
            "                                                                 batch_normalization_397[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_397 (Activation)     (None, 16, 16, 32)   0           add_196[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_407 (Conv2D)             (None, 16, 16, 32)   9248        activation_397[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_398 (BatchN (None, 16, 16, 32)   128         conv2d_407[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_398 (Activation)     (None, 16, 16, 32)   0           batch_normalization_398[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_408 (Conv2D)             (None, 16, 16, 32)   9248        activation_398[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_399 (BatchN (None, 16, 16, 32)   128         conv2d_408[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_197 (Add)                   (None, 16, 16, 32)   0           activation_397[0][0]             \n",
            "                                                                 batch_normalization_399[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_399 (Activation)     (None, 16, 16, 32)   0           add_197[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_409 (Conv2D)             (None, 16, 16, 32)   9248        activation_399[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_400 (BatchN (None, 16, 16, 32)   128         conv2d_409[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_400 (Activation)     (None, 16, 16, 32)   0           batch_normalization_400[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_410 (Conv2D)             (None, 16, 16, 32)   9248        activation_400[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_401 (BatchN (None, 16, 16, 32)   128         conv2d_410[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_198 (Add)                   (None, 16, 16, 32)   0           activation_399[0][0]             \n",
            "                                                                 batch_normalization_401[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_401 (Activation)     (None, 16, 16, 32)   0           add_198[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_411 (Conv2D)             (None, 16, 16, 32)   9248        activation_401[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_402 (BatchN (None, 16, 16, 32)   128         conv2d_411[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_402 (Activation)     (None, 16, 16, 32)   0           batch_normalization_402[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_412 (Conv2D)             (None, 16, 16, 32)   9248        activation_402[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_403 (BatchN (None, 16, 16, 32)   128         conv2d_412[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_199 (Add)                   (None, 16, 16, 32)   0           activation_401[0][0]             \n",
            "                                                                 batch_normalization_403[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_403 (Activation)     (None, 16, 16, 32)   0           add_199[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_413 (Conv2D)             (None, 16, 16, 32)   9248        activation_403[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_404 (BatchN (None, 16, 16, 32)   128         conv2d_413[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_404 (Activation)     (None, 16, 16, 32)   0           batch_normalization_404[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_414 (Conv2D)             (None, 16, 16, 32)   9248        activation_404[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_405 (BatchN (None, 16, 16, 32)   128         conv2d_414[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_200 (Add)                   (None, 16, 16, 32)   0           activation_403[0][0]             \n",
            "                                                                 batch_normalization_405[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_405 (Activation)     (None, 16, 16, 32)   0           add_200[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_415 (Conv2D)             (None, 16, 16, 32)   9248        activation_405[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_406 (BatchN (None, 16, 16, 32)   128         conv2d_415[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_406 (Activation)     (None, 16, 16, 32)   0           batch_normalization_406[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_416 (Conv2D)             (None, 16, 16, 32)   9248        activation_406[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_407 (BatchN (None, 16, 16, 32)   128         conv2d_416[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_201 (Add)                   (None, 16, 16, 32)   0           activation_405[0][0]             \n",
            "                                                                 batch_normalization_407[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_407 (Activation)     (None, 16, 16, 32)   0           add_201[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_417 (Conv2D)             (None, 16, 16, 32)   9248        activation_407[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_408 (BatchN (None, 16, 16, 32)   128         conv2d_417[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_408 (Activation)     (None, 16, 16, 32)   0           batch_normalization_408[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_418 (Conv2D)             (None, 16, 16, 32)   9248        activation_408[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_409 (BatchN (None, 16, 16, 32)   128         conv2d_418[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_202 (Add)                   (None, 16, 16, 32)   0           activation_407[0][0]             \n",
            "                                                                 batch_normalization_409[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_409 (Activation)     (None, 16, 16, 32)   0           add_202[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_419 (Conv2D)             (None, 16, 16, 32)   9248        activation_409[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_410 (BatchN (None, 16, 16, 32)   128         conv2d_419[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_410 (Activation)     (None, 16, 16, 32)   0           batch_normalization_410[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_420 (Conv2D)             (None, 16, 16, 32)   9248        activation_410[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_411 (BatchN (None, 16, 16, 32)   128         conv2d_420[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_203 (Add)                   (None, 16, 16, 32)   0           activation_409[0][0]             \n",
            "                                                                 batch_normalization_411[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_411 (Activation)     (None, 16, 16, 32)   0           add_203[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_421 (Conv2D)             (None, 16, 16, 32)   9248        activation_411[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_412 (BatchN (None, 16, 16, 32)   128         conv2d_421[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_412 (Activation)     (None, 16, 16, 32)   0           batch_normalization_412[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_422 (Conv2D)             (None, 16, 16, 32)   9248        activation_412[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_413 (BatchN (None, 16, 16, 32)   128         conv2d_422[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_204 (Add)                   (None, 16, 16, 32)   0           activation_411[0][0]             \n",
            "                                                                 batch_normalization_413[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_413 (Activation)     (None, 16, 16, 32)   0           add_204[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_423 (Conv2D)             (None, 16, 16, 32)   9248        activation_413[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_414 (BatchN (None, 16, 16, 32)   128         conv2d_423[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_414 (Activation)     (None, 16, 16, 32)   0           batch_normalization_414[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_424 (Conv2D)             (None, 16, 16, 32)   9248        activation_414[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_415 (BatchN (None, 16, 16, 32)   128         conv2d_424[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_205 (Add)                   (None, 16, 16, 32)   0           activation_413[0][0]             \n",
            "                                                                 batch_normalization_415[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_415 (Activation)     (None, 16, 16, 32)   0           add_205[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_425 (Conv2D)             (None, 16, 16, 32)   9248        activation_415[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_416 (BatchN (None, 16, 16, 32)   128         conv2d_425[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_416 (Activation)     (None, 16, 16, 32)   0           batch_normalization_416[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_426 (Conv2D)             (None, 16, 16, 32)   9248        activation_416[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_417 (BatchN (None, 16, 16, 32)   128         conv2d_426[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_206 (Add)                   (None, 16, 16, 32)   0           activation_415[0][0]             \n",
            "                                                                 batch_normalization_417[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_417 (Activation)     (None, 16, 16, 32)   0           add_206[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_427 (Conv2D)             (None, 16, 16, 32)   9248        activation_417[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_418 (BatchN (None, 16, 16, 32)   128         conv2d_427[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_418 (Activation)     (None, 16, 16, 32)   0           batch_normalization_418[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_428 (Conv2D)             (None, 16, 16, 32)   9248        activation_418[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_419 (BatchN (None, 16, 16, 32)   128         conv2d_428[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_207 (Add)                   (None, 16, 16, 32)   0           activation_417[0][0]             \n",
            "                                                                 batch_normalization_419[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_419 (Activation)     (None, 16, 16, 32)   0           add_207[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_429 (Conv2D)             (None, 16, 16, 32)   9248        activation_419[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_420 (BatchN (None, 16, 16, 32)   128         conv2d_429[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_420 (Activation)     (None, 16, 16, 32)   0           batch_normalization_420[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_430 (Conv2D)             (None, 16, 16, 32)   9248        activation_420[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_421 (BatchN (None, 16, 16, 32)   128         conv2d_430[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_208 (Add)                   (None, 16, 16, 32)   0           activation_419[0][0]             \n",
            "                                                                 batch_normalization_421[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_421 (Activation)     (None, 16, 16, 32)   0           add_208[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_431 (Conv2D)             (None, 16, 16, 32)   9248        activation_421[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_422 (BatchN (None, 16, 16, 32)   128         conv2d_431[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_422 (Activation)     (None, 16, 16, 32)   0           batch_normalization_422[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_432 (Conv2D)             (None, 16, 16, 32)   9248        activation_422[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_423 (BatchN (None, 16, 16, 32)   128         conv2d_432[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_209 (Add)                   (None, 16, 16, 32)   0           activation_421[0][0]             \n",
            "                                                                 batch_normalization_423[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_423 (Activation)     (None, 16, 16, 32)   0           add_209[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_433 (Conv2D)             (None, 16, 16, 32)   9248        activation_423[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_424 (BatchN (None, 16, 16, 32)   128         conv2d_433[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_424 (Activation)     (None, 16, 16, 32)   0           batch_normalization_424[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_434 (Conv2D)             (None, 16, 16, 32)   9248        activation_424[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_425 (BatchN (None, 16, 16, 32)   128         conv2d_434[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_210 (Add)                   (None, 16, 16, 32)   0           activation_423[0][0]             \n",
            "                                                                 batch_normalization_425[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_425 (Activation)     (None, 16, 16, 32)   0           add_210[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_435 (Conv2D)             (None, 16, 16, 32)   9248        activation_425[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_426 (BatchN (None, 16, 16, 32)   128         conv2d_435[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_426 (Activation)     (None, 16, 16, 32)   0           batch_normalization_426[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_436 (Conv2D)             (None, 16, 16, 32)   9248        activation_426[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_427 (BatchN (None, 16, 16, 32)   128         conv2d_436[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_211 (Add)                   (None, 16, 16, 32)   0           activation_425[0][0]             \n",
            "                                                                 batch_normalization_427[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_427 (Activation)     (None, 16, 16, 32)   0           add_211[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_437 (Conv2D)             (None, 16, 16, 32)   9248        activation_427[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_428 (BatchN (None, 16, 16, 32)   128         conv2d_437[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_428 (Activation)     (None, 16, 16, 32)   0           batch_normalization_428[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_438 (Conv2D)             (None, 16, 16, 32)   9248        activation_428[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_429 (BatchN (None, 16, 16, 32)   128         conv2d_438[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_212 (Add)                   (None, 16, 16, 32)   0           activation_427[0][0]             \n",
            "                                                                 batch_normalization_429[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_429 (Activation)     (None, 16, 16, 32)   0           add_212[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_439 (Conv2D)             (None, 16, 16, 32)   9248        activation_429[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_430 (BatchN (None, 16, 16, 32)   128         conv2d_439[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_430 (Activation)     (None, 16, 16, 32)   0           batch_normalization_430[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_440 (Conv2D)             (None, 16, 16, 32)   9248        activation_430[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_431 (BatchN (None, 16, 16, 32)   128         conv2d_440[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_213 (Add)                   (None, 16, 16, 32)   0           activation_429[0][0]             \n",
            "                                                                 batch_normalization_431[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_431 (Activation)     (None, 16, 16, 32)   0           add_213[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_441 (Conv2D)             (None, 16, 16, 32)   9248        activation_431[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_432 (BatchN (None, 16, 16, 32)   128         conv2d_441[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_432 (Activation)     (None, 16, 16, 32)   0           batch_normalization_432[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_442 (Conv2D)             (None, 16, 16, 32)   9248        activation_432[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_433 (BatchN (None, 16, 16, 32)   128         conv2d_442[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_214 (Add)                   (None, 16, 16, 32)   0           activation_431[0][0]             \n",
            "                                                                 batch_normalization_433[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_433 (Activation)     (None, 16, 16, 32)   0           add_214[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_443 (Conv2D)             (None, 16, 16, 32)   9248        activation_433[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_434 (BatchN (None, 16, 16, 32)   128         conv2d_443[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_434 (Activation)     (None, 16, 16, 32)   0           batch_normalization_434[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_444 (Conv2D)             (None, 16, 16, 32)   9248        activation_434[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_435 (BatchN (None, 16, 16, 32)   128         conv2d_444[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_215 (Add)                   (None, 16, 16, 32)   0           activation_433[0][0]             \n",
            "                                                                 batch_normalization_435[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_435 (Activation)     (None, 16, 16, 32)   0           add_215[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_445 (Conv2D)             (None, 16, 16, 32)   9248        activation_435[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_436 (BatchN (None, 16, 16, 32)   128         conv2d_445[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_436 (Activation)     (None, 16, 16, 32)   0           batch_normalization_436[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_446 (Conv2D)             (None, 16, 16, 32)   9248        activation_436[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_437 (BatchN (None, 16, 16, 32)   128         conv2d_446[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_216 (Add)                   (None, 16, 16, 32)   0           activation_435[0][0]             \n",
            "                                                                 batch_normalization_437[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_437 (Activation)     (None, 16, 16, 32)   0           add_216[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_447 (Conv2D)             (None, 8, 8, 64)     18496       activation_437[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_438 (BatchN (None, 8, 8, 64)     256         conv2d_447[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_438 (Activation)     (None, 8, 8, 64)     0           batch_normalization_438[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_448 (Conv2D)             (None, 8, 8, 64)     36928       activation_438[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_449 (Conv2D)             (None, 8, 8, 64)     2112        activation_437[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_439 (BatchN (None, 8, 8, 64)     256         conv2d_448[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_217 (Add)                   (None, 8, 8, 64)     0           conv2d_449[0][0]                 \n",
            "                                                                 batch_normalization_439[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_439 (Activation)     (None, 8, 8, 64)     0           add_217[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_450 (Conv2D)             (None, 8, 8, 64)     36928       activation_439[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_440 (BatchN (None, 8, 8, 64)     256         conv2d_450[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_440 (Activation)     (None, 8, 8, 64)     0           batch_normalization_440[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_451 (Conv2D)             (None, 8, 8, 64)     36928       activation_440[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_441 (BatchN (None, 8, 8, 64)     256         conv2d_451[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_218 (Add)                   (None, 8, 8, 64)     0           activation_439[0][0]             \n",
            "                                                                 batch_normalization_441[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_441 (Activation)     (None, 8, 8, 64)     0           add_218[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_452 (Conv2D)             (None, 8, 8, 64)     36928       activation_441[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_442 (BatchN (None, 8, 8, 64)     256         conv2d_452[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_442 (Activation)     (None, 8, 8, 64)     0           batch_normalization_442[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_453 (Conv2D)             (None, 8, 8, 64)     36928       activation_442[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_443 (BatchN (None, 8, 8, 64)     256         conv2d_453[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_219 (Add)                   (None, 8, 8, 64)     0           activation_441[0][0]             \n",
            "                                                                 batch_normalization_443[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_443 (Activation)     (None, 8, 8, 64)     0           add_219[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_454 (Conv2D)             (None, 8, 8, 64)     36928       activation_443[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_444 (BatchN (None, 8, 8, 64)     256         conv2d_454[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_444 (Activation)     (None, 8, 8, 64)     0           batch_normalization_444[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_455 (Conv2D)             (None, 8, 8, 64)     36928       activation_444[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_445 (BatchN (None, 8, 8, 64)     256         conv2d_455[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_220 (Add)                   (None, 8, 8, 64)     0           activation_443[0][0]             \n",
            "                                                                 batch_normalization_445[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_445 (Activation)     (None, 8, 8, 64)     0           add_220[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_456 (Conv2D)             (None, 8, 8, 64)     36928       activation_445[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_446 (BatchN (None, 8, 8, 64)     256         conv2d_456[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_446 (Activation)     (None, 8, 8, 64)     0           batch_normalization_446[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_457 (Conv2D)             (None, 8, 8, 64)     36928       activation_446[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_447 (BatchN (None, 8, 8, 64)     256         conv2d_457[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_221 (Add)                   (None, 8, 8, 64)     0           activation_445[0][0]             \n",
            "                                                                 batch_normalization_447[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_447 (Activation)     (None, 8, 8, 64)     0           add_221[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_458 (Conv2D)             (None, 8, 8, 64)     36928       activation_447[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_448 (BatchN (None, 8, 8, 64)     256         conv2d_458[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_448 (Activation)     (None, 8, 8, 64)     0           batch_normalization_448[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_459 (Conv2D)             (None, 8, 8, 64)     36928       activation_448[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_449 (BatchN (None, 8, 8, 64)     256         conv2d_459[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_222 (Add)                   (None, 8, 8, 64)     0           activation_447[0][0]             \n",
            "                                                                 batch_normalization_449[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_449 (Activation)     (None, 8, 8, 64)     0           add_222[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_460 (Conv2D)             (None, 8, 8, 64)     36928       activation_449[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_450 (BatchN (None, 8, 8, 64)     256         conv2d_460[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_450 (Activation)     (None, 8, 8, 64)     0           batch_normalization_450[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_461 (Conv2D)             (None, 8, 8, 64)     36928       activation_450[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_451 (BatchN (None, 8, 8, 64)     256         conv2d_461[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_223 (Add)                   (None, 8, 8, 64)     0           activation_449[0][0]             \n",
            "                                                                 batch_normalization_451[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_451 (Activation)     (None, 8, 8, 64)     0           add_223[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_462 (Conv2D)             (None, 8, 8, 64)     36928       activation_451[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_452 (BatchN (None, 8, 8, 64)     256         conv2d_462[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_452 (Activation)     (None, 8, 8, 64)     0           batch_normalization_452[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_463 (Conv2D)             (None, 8, 8, 64)     36928       activation_452[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_453 (BatchN (None, 8, 8, 64)     256         conv2d_463[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_224 (Add)                   (None, 8, 8, 64)     0           activation_451[0][0]             \n",
            "                                                                 batch_normalization_453[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_453 (Activation)     (None, 8, 8, 64)     0           add_224[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_464 (Conv2D)             (None, 8, 8, 64)     36928       activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_454 (BatchN (None, 8, 8, 64)     256         conv2d_464[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_454 (Activation)     (None, 8, 8, 64)     0           batch_normalization_454[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_465 (Conv2D)             (None, 8, 8, 64)     36928       activation_454[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_455 (BatchN (None, 8, 8, 64)     256         conv2d_465[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_225 (Add)                   (None, 8, 8, 64)     0           activation_453[0][0]             \n",
            "                                                                 batch_normalization_455[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_455 (Activation)     (None, 8, 8, 64)     0           add_225[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_466 (Conv2D)             (None, 8, 8, 64)     36928       activation_455[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_456 (BatchN (None, 8, 8, 64)     256         conv2d_466[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_456 (Activation)     (None, 8, 8, 64)     0           batch_normalization_456[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_467 (Conv2D)             (None, 8, 8, 64)     36928       activation_456[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_457 (BatchN (None, 8, 8, 64)     256         conv2d_467[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_226 (Add)                   (None, 8, 8, 64)     0           activation_455[0][0]             \n",
            "                                                                 batch_normalization_457[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_457 (Activation)     (None, 8, 8, 64)     0           add_226[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_468 (Conv2D)             (None, 8, 8, 64)     36928       activation_457[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_458 (BatchN (None, 8, 8, 64)     256         conv2d_468[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_458 (Activation)     (None, 8, 8, 64)     0           batch_normalization_458[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_469 (Conv2D)             (None, 8, 8, 64)     36928       activation_458[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_459 (BatchN (None, 8, 8, 64)     256         conv2d_469[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_227 (Add)                   (None, 8, 8, 64)     0           activation_457[0][0]             \n",
            "                                                                 batch_normalization_459[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_459 (Activation)     (None, 8, 8, 64)     0           add_227[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_470 (Conv2D)             (None, 8, 8, 64)     36928       activation_459[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_460 (BatchN (None, 8, 8, 64)     256         conv2d_470[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_460 (Activation)     (None, 8, 8, 64)     0           batch_normalization_460[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_471 (Conv2D)             (None, 8, 8, 64)     36928       activation_460[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_461 (BatchN (None, 8, 8, 64)     256         conv2d_471[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_228 (Add)                   (None, 8, 8, 64)     0           activation_459[0][0]             \n",
            "                                                                 batch_normalization_461[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_461 (Activation)     (None, 8, 8, 64)     0           add_228[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_472 (Conv2D)             (None, 8, 8, 64)     36928       activation_461[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_462 (BatchN (None, 8, 8, 64)     256         conv2d_472[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_462 (Activation)     (None, 8, 8, 64)     0           batch_normalization_462[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_473 (Conv2D)             (None, 8, 8, 64)     36928       activation_462[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_463 (BatchN (None, 8, 8, 64)     256         conv2d_473[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_229 (Add)                   (None, 8, 8, 64)     0           activation_461[0][0]             \n",
            "                                                                 batch_normalization_463[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_463 (Activation)     (None, 8, 8, 64)     0           add_229[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_474 (Conv2D)             (None, 8, 8, 64)     36928       activation_463[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_464 (BatchN (None, 8, 8, 64)     256         conv2d_474[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_464 (Activation)     (None, 8, 8, 64)     0           batch_normalization_464[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_475 (Conv2D)             (None, 8, 8, 64)     36928       activation_464[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_465 (BatchN (None, 8, 8, 64)     256         conv2d_475[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_230 (Add)                   (None, 8, 8, 64)     0           activation_463[0][0]             \n",
            "                                                                 batch_normalization_465[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_465 (Activation)     (None, 8, 8, 64)     0           add_230[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_476 (Conv2D)             (None, 8, 8, 64)     36928       activation_465[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_466 (BatchN (None, 8, 8, 64)     256         conv2d_476[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_466 (Activation)     (None, 8, 8, 64)     0           batch_normalization_466[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_477 (Conv2D)             (None, 8, 8, 64)     36928       activation_466[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_467 (BatchN (None, 8, 8, 64)     256         conv2d_477[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_231 (Add)                   (None, 8, 8, 64)     0           activation_465[0][0]             \n",
            "                                                                 batch_normalization_467[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_467 (Activation)     (None, 8, 8, 64)     0           add_231[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_478 (Conv2D)             (None, 8, 8, 64)     36928       activation_467[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_468 (BatchN (None, 8, 8, 64)     256         conv2d_478[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_468 (Activation)     (None, 8, 8, 64)     0           batch_normalization_468[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_479 (Conv2D)             (None, 8, 8, 64)     36928       activation_468[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_469 (BatchN (None, 8, 8, 64)     256         conv2d_479[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_232 (Add)                   (None, 8, 8, 64)     0           activation_467[0][0]             \n",
            "                                                                 batch_normalization_469[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_469 (Activation)     (None, 8, 8, 64)     0           add_232[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_480 (Conv2D)             (None, 8, 8, 64)     36928       activation_469[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_470 (BatchN (None, 8, 8, 64)     256         conv2d_480[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_470 (Activation)     (None, 8, 8, 64)     0           batch_normalization_470[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_481 (Conv2D)             (None, 8, 8, 64)     36928       activation_470[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_471 (BatchN (None, 8, 8, 64)     256         conv2d_481[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_233 (Add)                   (None, 8, 8, 64)     0           activation_469[0][0]             \n",
            "                                                                 batch_normalization_471[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_471 (Activation)     (None, 8, 8, 64)     0           add_233[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_482 (Conv2D)             (None, 8, 8, 64)     36928       activation_471[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_472 (BatchN (None, 8, 8, 64)     256         conv2d_482[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_472 (Activation)     (None, 8, 8, 64)     0           batch_normalization_472[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_483 (Conv2D)             (None, 8, 8, 64)     36928       activation_472[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_473 (BatchN (None, 8, 8, 64)     256         conv2d_483[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_234 (Add)                   (None, 8, 8, 64)     0           activation_471[0][0]             \n",
            "                                                                 batch_normalization_473[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_473 (Activation)     (None, 8, 8, 64)     0           add_234[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_484 (Conv2D)             (None, 8, 8, 64)     36928       activation_473[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_474 (BatchN (None, 8, 8, 64)     256         conv2d_484[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_474 (Activation)     (None, 8, 8, 64)     0           batch_normalization_474[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_485 (Conv2D)             (None, 8, 8, 64)     36928       activation_474[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_475 (BatchN (None, 8, 8, 64)     256         conv2d_485[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_235 (Add)                   (None, 8, 8, 64)     0           activation_473[0][0]             \n",
            "                                                                 batch_normalization_475[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_475 (Activation)     (None, 8, 8, 64)     0           add_235[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_486 (Conv2D)             (None, 8, 8, 64)     36928       activation_475[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_476 (BatchN (None, 8, 8, 64)     256         conv2d_486[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_476 (Activation)     (None, 8, 8, 64)     0           batch_normalization_476[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_487 (Conv2D)             (None, 8, 8, 64)     36928       activation_476[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_477 (BatchN (None, 8, 8, 64)     256         conv2d_487[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_236 (Add)                   (None, 8, 8, 64)     0           activation_475[0][0]             \n",
            "                                                                 batch_normalization_477[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_477 (Activation)     (None, 8, 8, 64)     0           add_236[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_488 (Conv2D)             (None, 8, 8, 64)     36928       activation_477[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_478 (BatchN (None, 8, 8, 64)     256         conv2d_488[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_478 (Activation)     (None, 8, 8, 64)     0           batch_normalization_478[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_489 (Conv2D)             (None, 8, 8, 64)     36928       activation_478[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_479 (BatchN (None, 8, 8, 64)     256         conv2d_489[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_237 (Add)                   (None, 8, 8, 64)     0           activation_477[0][0]             \n",
            "                                                                 batch_normalization_479[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_479 (Activation)     (None, 8, 8, 64)     0           add_237[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_490 (Conv2D)             (None, 8, 8, 64)     36928       activation_479[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_480 (BatchN (None, 8, 8, 64)     256         conv2d_490[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_480 (Activation)     (None, 8, 8, 64)     0           batch_normalization_480[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_491 (Conv2D)             (None, 8, 8, 64)     36928       activation_480[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_481 (BatchN (None, 8, 8, 64)     256         conv2d_491[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_238 (Add)                   (None, 8, 8, 64)     0           activation_479[0][0]             \n",
            "                                                                 batch_normalization_481[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_481 (Activation)     (None, 8, 8, 64)     0           add_238[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_492 (Conv2D)             (None, 8, 8, 64)     36928       activation_481[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_482 (BatchN (None, 8, 8, 64)     256         conv2d_492[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_482 (Activation)     (None, 8, 8, 64)     0           batch_normalization_482[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_493 (Conv2D)             (None, 8, 8, 64)     36928       activation_482[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_483 (BatchN (None, 8, 8, 64)     256         conv2d_493[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_239 (Add)                   (None, 8, 8, 64)     0           activation_481[0][0]             \n",
            "                                                                 batch_normalization_483[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_483 (Activation)     (None, 8, 8, 64)     0           add_239[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_494 (Conv2D)             (None, 8, 8, 64)     36928       activation_483[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_484 (BatchN (None, 8, 8, 64)     256         conv2d_494[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_484 (Activation)     (None, 8, 8, 64)     0           batch_normalization_484[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_495 (Conv2D)             (None, 8, 8, 64)     36928       activation_484[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_485 (BatchN (None, 8, 8, 64)     256         conv2d_495[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_240 (Add)                   (None, 8, 8, 64)     0           activation_483[0][0]             \n",
            "                                                                 batch_normalization_485[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_485 (Activation)     (None, 8, 8, 64)     0           add_240[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_496 (Conv2D)             (None, 8, 8, 64)     36928       activation_485[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_486 (BatchN (None, 8, 8, 64)     256         conv2d_496[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_486 (Activation)     (None, 8, 8, 64)     0           batch_normalization_486[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_497 (Conv2D)             (None, 8, 8, 64)     36928       activation_486[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_487 (BatchN (None, 8, 8, 64)     256         conv2d_497[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_241 (Add)                   (None, 8, 8, 64)     0           activation_485[0][0]             \n",
            "                                                                 batch_normalization_487[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_487 (Activation)     (None, 8, 8, 64)     0           add_241[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_498 (Conv2D)             (None, 8, 8, 64)     36928       activation_487[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_488 (BatchN (None, 8, 8, 64)     256         conv2d_498[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_488 (Activation)     (None, 8, 8, 64)     0           batch_normalization_488[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_499 (Conv2D)             (None, 8, 8, 64)     36928       activation_488[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_489 (BatchN (None, 8, 8, 64)     256         conv2d_499[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_242 (Add)                   (None, 8, 8, 64)     0           activation_487[0][0]             \n",
            "                                                                 batch_normalization_489[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_489 (Activation)     (None, 8, 8, 64)     0           add_242[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_500 (Conv2D)             (None, 8, 8, 64)     36928       activation_489[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_490 (BatchN (None, 8, 8, 64)     256         conv2d_500[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_490 (Activation)     (None, 8, 8, 64)     0           batch_normalization_490[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_501 (Conv2D)             (None, 8, 8, 64)     36928       activation_490[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_491 (BatchN (None, 8, 8, 64)     256         conv2d_501[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_243 (Add)                   (None, 8, 8, 64)     0           activation_489[0][0]             \n",
            "                                                                 batch_normalization_491[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_491 (Activation)     (None, 8, 8, 64)     0           add_243[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 1, 1, 64)     0           activation_491[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 64)           0           average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 10)           650         flatten_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,623,754\n",
            "Trainable params: 2,611,626\n",
            "Non-trainable params: 12,128\n",
            "__________________________________________________________________________________________________\n",
            "ResNet164v1\n",
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-6d6d898de86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    315\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   3023\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3025\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients_v2\u001b[0;34m(ys, xs, grad_ys, name, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    303\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0mstop_gradient_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     reachable_to_ops, pending_count, loop_state = _PendingCount(\n\u001b[0;32m--> 551\u001b[0;31m         to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs_set)\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;31m# Iterate over the collected ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_PendingCount\u001b[0;34m(to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs_set)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;31m# Mark reachable ops from from_ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mreached_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0m_MarkReachedOps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreached_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m   \u001b[0;31m# X in reached_ops iff X is reachable from from_ops by a path of zero or more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;31m# backpropagatable tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MarkReachedOps\u001b[0;34m(from_ops, reached_ops, func_graphs)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mreached_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_IsBackpropagatable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m           \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Consumers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_IsBackpropagatable\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_IsBackpropagatable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsTrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop_util.py\u001b[0m in \u001b[0;36mIsTrainable\u001b[0;34m(tensor_or_dtype)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_or_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   return dtype.base_dtype in (dtypes.float16, dtypes.float32, dtypes.float64,\n\u001b[0m\u001b[1;32m     32\u001b[0m                               \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                               dtypes.resource, dtypes.variant)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mbase_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbase_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;34m\"\"\"Returns a non-reference `DType` based on this `DType`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ref_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m_is_ref_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_is_ref_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m\"\"\"Returns `True` if this `DType` represents a reference type.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-UqtzHxXLID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "98e991e2-b736-48f4-9560-f9b5ed02d377"
      },
      "source": [
        "\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "#plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f25209bc828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFooqD60wZli",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "86068c88-fc3c-4e0b-c3be-81b4862750ba"
      },
      "source": [
        "plt.plot(test_error, label = 'Test error Resnet20v1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Error')\n",
        "plt.xlim([0,200])\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f247ab56780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyTS4w-wyxog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5f5550e3-344f-4445-b2ca-f521a001bfb8"
      },
      "source": [
        "train_error=[]\n",
        "\n",
        "for t in history.history['accuracy']:\n",
        "  train_error.append(1-t)\n",
        "\n",
        "plt.plot(train_error, label = 'Cross entropy Train loss Resnet20v1')\n",
        "plt.xlim([0,200])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross entropy Train loss')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2477e8aa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMSmFWyj8-73",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "d699cf1b-cd84-476a-d40e-17815c8f9f57"
      },
      "source": [
        "plt.plot(history.history['val_loss'], label = 'Cross entropy Test loss Resnet20v1')\n",
        "plt.xlim([0,300])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross entropy Test loss')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2477eb3908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRZOtLJ9VHay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f992b406-f928-4e46-999c-c095aede9163"
      },
      "source": [
        "test_error=[]\n",
        "print(type(history.history['val_accuracy']))\n",
        "for t in history.history['val_accuracy']:\n",
        "  test_error.append(1-t)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIRmec_sGqpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = make_model()  # define our model# prepare model for fitting (loss, optimizer, etc)\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # we train 10-way classification\n",
        "    optimizer=keras.optimizers.adamax(lr=INIT_LR),  # for SGD\n",
        "    metrics=['accuracy']  # report accuracy during training\n",
        "\n",
        ")\n",
        "\n",
        "def lr_scheduler(epoch):\n",
        "    return INIT_LR * 0.9 ** epoch\n",
        "class LrHistory(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs={}):\n",
        "    print(\"Learning rate:\", K.get_value(model.optimizer.lr))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQxWegvSN4mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX5meyq3OAaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jtho0a4YOaDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from keras.callbacks import TensorBoard\n",
        "tbCallBack = TensorBoard(log_dir='./log/cnn', histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         write_images=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCIq-L1rOb65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "history = model.fit(\n",
        "    x_train2, y_train2,  # prepared data\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(x_test2, y_test2),\n",
        "    shuffle=True,\n",
        "    \n",
        "    verbose=0,\n",
        "    callbacks =[keras.callbacks.LearningRateScheduler(lr_scheduler),\n",
        "                 LrHistory(),tbCallBack],\n",
        "    initial_epoch= 0\n",
        ")\n",
        "print('train acc',history.history['accuracy'])\n",
        "print('valid acc',history.history['val_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EhSrxNGnwOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_test = model.predict_proba(x_test2)\n",
        "y_pred_test_classes = np.argmax(y_pred_test, axis=1)\n",
        "y_pred_test_max_probas = np.max(y_pred_test, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn8DB5NLOrHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INIT_LR = 5e-3  # initial learning rate\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xNKXi-Gy3RO-"
      },
      "source": [
        "As you can see, our (4, 4, 64) outputs were flattened into vectors of shape (1024) before going through two Dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P3odqfHP4M67"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MdDzI75PUXrG",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='SGD',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=10, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jKgyC5K_4O0d"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gtyDF0MKUcM7",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0LvwaKhtUdOo",
        "colab": {}
      },
      "source": [
        "print(test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8cfJ8AR03gT5"
      },
      "source": [
        "Our simple CNN has achieved a test accuracy of over 70%. Not bad for a few lines of code! For another CNN style, see an example using the Keras subclassing API and a `tf.GradientTape` [here](https://www.tensorflow.org/tutorials/quickstart/advanced)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Bh52PX4md-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}